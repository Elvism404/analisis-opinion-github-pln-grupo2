tipo,numero,titulo,texto
pull_request,33264,MNT remove unused function in PowerTransformer,"#### Reference Issues/PRs
None

#### What does this implement/fix? Explain your changes.
This PR removes an unused function in class PowerTransformer.

#### AI usage disclosure
None"
pull_request,33263,Add array API support for `FeatureUnion`,"While working on an array API demo, I realized that `FeatureUnion` did not support array API even when the underlying transformers do. Here is a fix.

#### AI usage disclosure

I used AI assistance for:
- [x] Code generation (e.g., when writing an implementation or fixing a bug)
- [x] Test/benchmark generation
- [ ] Documentation (including examples)
- [ ] Research and understanding

I used cursor but manually reviewed and improved the generated code.

"
pull_request,33262,Fix the eigsh call in spectral embedding,"Fixes #33242 

This fixes the way `eigsh()` is called in spectral embedding. As a result, it gets faster:

```python
import numpy as np
from sklearn.manifold import SpectralEmbedding
from sklearn.datasets import make_swiss_roll

X, _ = make_swiss_roll(n_samples=10_000)
%time Z = SpectralEmbedding(eigen_solver=""arpack"", affinity=""rbf"", eigen_tol=0).fit_transform(X)
```
On my machine, this runs in 24 seconds on current main, and in 12 second after my fix. I am using `affinity=""rbf""` here to get a dense `N x N` affinity matrix (with sparse matrix it's much faster)."
comment,33262,,"Oh wow, some tests fail but only in some environments (on my local machine all tests pass). At the moment I have no idea why."
comment,33262,,You can set `SKLEARN_TESTS_GLOBAL_RANDOM_SEED=all` to run the `global_random_seed`-parametrized tests with all admissible RNG seeds.
comment,33262,,"@ogrisel Thanks, I did not know about it. But this does not seem to be the problem here. On my machine
```
SKLEARN_TESTS_GLOBAL_RANDOM_SEED=""all"" pytest sklearn/cluster/tests/test_spectral.py 
```
passes for all seeds, but here we see some failing tests in `test_spectral.py` in some environments. Also, some tests in `test_spectral_embedding.py` are also failing in some environments but that file does not use `global_random_seed` at all...

This must be due to some ARPACK-related differences, right? I only changed one single thing that only affects one `eigsh` call..."
comment,33262,,"OK it seems the issue was due to conversion between different `scipy.sparse` formats (I found similar thing discussed in a closed PR), so I rewrote it to avoid such conversion."
pull_request,33261,fix(doc): fix mutable default arguments in doi_role.py,"#### Reference Issues/PRs
Fixes mutable default arguments in `doc/sphinxext/doi_role.py`.

#### What does this implement/fix? Explain your changes.
This PR replaces mutable default arguments (`options={}` and `content=[]`) with `None` in the `reference_role` function.

In Python, default arguments are evaluated once at definition time. Using mutable types (like lists or dicts) as default arguments can lead to state persisting across function calls, potentially causing unexpected side effects.

I have updated the function signature to use `None` and added the standard initialization check:
```python
if options is None:
    options = {}
if content is None:
    content = []"
pull_request,33260,NMF error with SciPy dev,"<!--
üôå Thanks for contributing a pull request!

üëÄ Please ensure you have taken a look at the contribution guidelines:
https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md

‚úÖ In particular following the pull request checklist will increase the likelihood
of having maintainers review your PR:
https://scikit-learn.org/dev/developers/contributing.html#pull-request-checklist

üìã If your PR is likely to affect users, you will need to add a changelog entry
describing your PR changes, see:
https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/README.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
The following code raise the error with scipy dev:
```python
from sklearn.datasets import make_low_rank_matrix
from sklearn.decomposition import NMF

X = make_low_rank_matrix(n_samples=20, n_features=100, effective_rank=5, random_state=123) + 1
nmf = NMF(n_components=5).fit_transform(X)
```
The error is:
```python
Traceback (most recent call last):
  File ""~\nmf_failure.py"", line 5, in <module>
    nmf = NMF(n_components=5).fit_transform(X)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""~\miniforge3\envs\dep_dev\Lib\site-packages\sklearn\utils\_set_output.py"", line 316, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""~\miniforge3\envs\dep_dev\Lib\site-packages\sklearn\base.py"", line 1336, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""~\miniforge3\envs\dep_dev\Lib\site-packages\sklearn\decomposition\_nmf.py"", line 1619, in fit_transform
    W, H, n_iter = self._fit_transform(X, W=W, H=H)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""~\miniforge3\envs\dep_dev\Lib\site-packages\sklearn\decomposition\_nmf.py"", line 1687, in _fit_transform
    W, H, n_iter = _fit_coordinate_descent(
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""~\miniforge3\envs\dep_dev\Lib\site-packages\sklearn\decomposition\_nmf.py"", line 493, in _fit_coordinate_descent
    violation += _update_coordinate_descent(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""~\miniforge3\envs\dep_dev\Lib\site-packages\sklearn\decomposition\_nmf.py"", line 396, in _update_coordinate_descent
    return _update_cdnmf_fast(W, HHt, XHt, permutation)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""sklearn/decomposition/_cdnmf_fast.pyx"", line 8, in sklearn.decomposition._cdnmf_fast._update_cdnmf_fast
    def _update_cdnmf_fast(floating[:, ::1] W, floating[:, :] HHt,
  File ""<stringsource>"", line 664, in View.MemoryView.memoryview_cwrapper
  File ""<stringsource>"", line 352, in View.MemoryView.memoryview.__cinit__
ValueError: ndarray is not C-contiguous
```

As discussed in https://github.com/scipy/scipy/issues/24538, this is related to a change in the C/F ordering in the output of `scipy.linalg.svd`.

#### What does this implement/fix? Explain your changes.

Specifying the order of the array at creating fixes the issue.

#### AI usage disclosure
<!--
If AI tools were involved in creating this PR, please check all boxes that apply
below and make sure that you adhere to our Automated Contributions Policy:
https://scikit-learn.org/dev/developers/contributing.html#automated-contributions-policy
-->
I used AI assistance for:
- [ ] Code generation (e.g., when writing an implementation or fixing a bug)
- [ ] Test/benchmark generation
- [ ] Documentation (including examples)
- [ ] Research and understanding


#### Any other comments?

I am surprised that this is not caught on CI?

<!--
Thank you for your patience. Changes to scikit-learn require careful
attention, but with limited maintainer time, not every contribution can be reviewed
quickly.
For more information and tips on improving your pull request, see:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
"
comment,33260,,"A scipy dev here. There's some discussion of the scipy perspective in https://github.com/scipy/scipy/issues/24538#issuecomment-3867035499. To summarize: scipy did indeed change the layout from F (in fact, whatever f2py was returning) to C in main (to-be-released scipy 1.18); for scipy itself it makes batched linalg a little more sane: in scipy==1.16.x doing SVD of a batch creates an array with C-ordered batch dimensions and F-ordered core dimensions. I don't believe scipy makes hard guarantees on what is the ordering though.

From a quick glance at the code, one potentially related issue is this branching: https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/utils/extmath.py#L608
AFAIU when array API dispatch is active and `xp==torch`, `_randomized_svd` outputs might very well be C ordered just because torch.linalg.svd likely produces C arrays or at least does not guarantee the ordering. Which, at least on the surface of it, argues for the fix made here: if something is fed to cython-implemented `double[:, ::1]` memoryview, the call site is responsible for making sure it is indeed correctly ordered."
pull_request,33259,Fix broken BNP Paribas logo link in documentation homepage,"The documentation homepage referenced a non-existent BNP Paribas logo (`bnp-paribas.png`). 

The correct asset already exists as `bnp-paribas.jpg`.

This change updates the reference so the logo renders correctly again."
pull_request,33258,ENH Support cardinality filtering in make_column_selector,"Supersedes #33101 (Reopening via new PR due to accidental closure).

This PR adds `min_cardinality` and `max_cardinality` parameters to `make_column_selector`.
It helps in filtering columns based on their number of unique values (e.g., identifying constant columns or high-cardinality categorical features)."
comment,33258,,"## ‚ùå Linting issues

This PR is introducing linting issues. Here's a summary of the issues. Note that you can avoid having linting issues by enabling `pre-commit` hooks. Instructions to enable them can be found [here](https://scikit-learn.org/dev/developers/development_setup.html#set-up-pre-commit).

You can see the details of the linting issues under the `lint` job [here](https://github.com/scikit-learn/scikit-learn/actions/runs/21921728313)

-----------------------------------------------
### `ruff check`

`ruff` detected issues. Please run `ruff check --fix --output-format=full` locally, fix the remaining issues, and push the changes. Here you can see the detected issues. Note that the installed `ruff` version is `ruff=0.12.2`.

<details>

```

sklearn/utils/tests/test_response.py:1:1: I001 [*] Import block is un-sorted or un-formatted
   |
 1 | / import warnings
 2 | |
 3 | | import numpy as np
 4 | | import pytest
 5 | |
 6 | | from sklearn.base import clone
 7 | | from sklearn.cluster import DBSCAN, KMeans
 8 | | from sklearn.datasets import (load_iris, make_classification,
 9 | |                               make_multilabel_classification)
10 | | from sklearn.ensemble import IsolationForest
11 | | from sklearn.linear_model import LinearRegression, LogisticRegression
12 | | from sklearn.multioutput import ClassifierChain
13 | | from sklearn.preprocessing import scale
14 | | from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
15 | | from sklearn.utils._response import (_get_response_values,
16 | |                                      _get_response_values_binary)
17 | | from sklearn.utils._testing import assert_allclose
   | |__________________________________________________^ I001
18 |
19 |   X, y = load_iris(return_X_y=True)
   |
   = help: Organize imports

Found 1 error.
[*] 1 fixable with the `--fix` option.
```

</details>

-----------------------------------------------
### `ruff format`

`ruff` detected issues. Please run `ruff format` locally and push the changes. Here you can see the detected issues. Note that the installed `ruff` version is `ruff=0.12.2`.

<details>

```

--- sklearn/utils/tests/test_response.py
+++ sklearn/utils/tests/test_response.py
@@ -5,15 +5,17 @@
 
 from sklearn.base import clone
 from sklearn.cluster import DBSCAN, KMeans
-from sklearn.datasets import (load_iris, make_classification,
-                              make_multilabel_classification)
+from sklearn.datasets import (
+    load_iris,
+    make_classification,
+    make_multilabel_classification,
+)
 from sklearn.ensemble import IsolationForest
 from sklearn.linear_model import LinearRegression, LogisticRegression
 from sklearn.multioutput import ClassifierChain
 from sklearn.preprocessing import scale
 from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
-from sklearn.utils._response import (_get_response_values,
-                                     _get_response_values_binary)
+from sklearn.utils._response import _get_response_values, _get_response_values_binary
 from sklearn.utils._testing import assert_allclose
 
 X, y = load_iris(return_X_y=True)

1 file would be reformatted, 931 files already formatted
```

</details>



<sub> _Generated for commit: [bd1fc5e](https://github.com/scikit-learn/scikit-learn/pull/33258/commits/bd1fc5e803d6ada7f3ce14b01c08054dcda3f7d7). Link to the linter CI: [here](https://github.com/scikit-learn/scikit-learn/actions/runs/21921728313)_ </sub>"
pull_request,33257,FIX avoid repeated third-party deprecation warnings in process-based Parallel,"## Summary

This PR reduces repeated third-party deprecation warnings in process-based parallel runs.

Related to #33230.

The issue can be reproduced with `permutation_importance(..., n_jobs=-1)` after importing `hyperopt`.
Before this change, the same `pkg_resources` deprecation warning is emitted many times from multiprocessing queue result loading.

## Root cause

`sklearn.utils.parallel.Parallel` propagates `warnings.filters` to workers.
Some filter categories are third-party warning class objects. Sending those classes to workers can trigger implicit module imports during unpickling, and those imports can emit warnings again.

## What changed

- In `sklearn/utils/parallel.py`:
  - Added `_pack_warning_filter` to serialize warning categories as `(module, qualname)` references.
  - Added `_unpack_warning_filter` to restore categories from already-loaded modules only.
  - Skips filter entries that would require implicit imports.
- Added regression test in `sklearn/utils/tests/test_parallel.py`:
  - `test_filter_warning_no_implicit_third_party_import_in_loky_workers`

## Validation

Ran locally:

```bash
python -m pytest --color=no sklearn/utils/tests/test_parallel.py -q
python -m pytest --color=no sklearn/inspection/tests/test_permutation_importance.py -q
```

Results:

- `16 passed, 2 skipped`
- `23 passed, 12 skipped`

In local reproduction, repeated warnings from `multiprocessing/queues.py:122` disappear after this patch, while the original third-party import warning remains once.

"
comment,33257,,"Update: This PR is ready for review.
Attention: AI was used to correct English grammar, vocabulary, and sentence structure."
issue,33256,Footer: BNP Paribas sponsor logo is missing (broken image) in footer,"On the scikit-learn documentation website footer, the BNP Paribas sponsor
logo does not render correctly.

Instead of the logo image, only the fallback text ""BNP"" is displayed,
which suggests the image asset is missing or failing to load.

Steps to reproduce:
1. Open https://scikit-learn.org/stable/
2. Scroll to the footer
<img width=""1717"" height=""850"" alt=""Image"" src=""https://github.com/user-attachments/assets/01012fde-e38c-4fa7-9b17-3ab22194c0d3"" />

<img width=""127"" height=""62"" alt=""Image"" src=""https://github.com/user-attachments/assets/9089e2fe-0705-4f1f-b609-640b460bb2b0"" />

3. Observe the BNP sponsor logo area

Expected behavior:
The BNP Paribas logo image should be displayed correctly.

Actual behavior:
Only the fallback text ""BNP"" is shown (broken or missing image).

Screenshot attached.
I am happy to investigate or submit a fix if this is confirmed."
comment,33256,,"Hi! 
I‚Äôd like to work on this. 
I‚Äôll investigate where the footer logo is defined in the docs theme, and submit a PR to fix the broken image."
comment,33256,,Thanks for picking this up! Appreciate you looking into it. Let me know if you need any more details or help testing.
comment,33256,,Can confirm image missing in dev docs as well.
comment,33256,,"Thanks for confirming

I‚Äôve pushed an update using bnp-small.png, consistent with the other sponsor images in the footer. 

This should fix the missing image in both stable and dev builds."
pull_request,33255,ENH: Add min_cardinality and max_cardinality to make_column_selector,"#### Reference Issues/PRs

Closes #15873. See also #22923.

#### What does this implement/fix?

Adds `min_cardinality` and `max_cardinality` parameters to `make_column_selector`, enabling cardinality-based column selection. This addresses the feature request in #15873 and incorporates the API feedback from maintainers on #22923 (using `min_cardinality`/`max_cardinality` instead of `cardinality`/`cardinality_threshold`).

**New parameters:**
- `min_cardinality`: minimum number of unique values a column must have (inclusive)
- `max_cardinality`: maximum number of unique values a column must have (inclusive)

Both parameters work with the existing `dtype_include`, `dtype_exclude`, and `pattern` filters (all criteria must match).

**Example usage:**
```python
# Select low-cardinality columns for one-hot encoding
make_column_selector(dtype_include=object, max_cardinality=10)

# Select high-cardinality columns for target encoding
make_column_selector(dtype_include=object, min_cardinality=11)
```

#### Changes
- `sklearn/compose/_column_transformer.py`: Added `min_cardinality` and `max_cardinality` params to `make_column_selector`
- `sklearn/compose/tests/test_column_transformer.py`: Added tests for cardinality filtering (standalone, combined with dtype, combined with pattern)"
pull_request,33254,DOC copy note on calibration to Brier score loss,"#### Reference Issues/PRs
None


#### What does this implement/fix? Explain your changes.
This PR copies the note on the Brier score decomposition and what it means for calibration from the calibration section of the user guide to the Brier score loss.

#### AI usage disclosure
None

#### Any other comments?
None"
pull_request,33253,DOC D2 Brier score aka scaled Brier score,"#### Reference Issues/PRs
None

#### What does this implement/fix? Explain your changes.
Small addition or clarification for the D2 Brier score.

#### AI usage disclosure
None

#### Any other comments?
None"
pull_request,33252,WIP: Gather sort functions in one file + use intro sort for `simultaneous_sort`,"#### Reference Issues/PRs

Fixes #33167

Follow-up/supersedes #33222 

#### What does this implement/fix? Explain your changes.

Why we need intro sort `simultaneous_sort`: see #33167 and #33222

And because intro sort was already implemented for decision it made sense to share common bits in a single file (to reduce the number of loc, but mainly to have one single place with the two very similar sort side by side with explanations about why they both exist).

#### AI usage: mostly no
"
pull_request,33251,DOC Add hint that example gallery images are clickable,"This PR implements the UI hint discussed in issue #30596.

### Changes
- Adds a hover box-shadow effect to clickable example gallery thumbnails
- Adds a small `[source]` label using `figure > a::after`

### Motivation
Clickable images currently look like normal static plots, so users often miss that they link to the full example code.

This makes the interaction clearer while keeping the UI minimal.
"
comment,33251,,"Thanks for tackling this! Could you post some before and after screenshots please? I think I know which part of the docs I need to look at to see this in action, but even so it would be good to have an explicit example. Even more so for people who have not followed the discussion in the issue but could review this PR."
comment,33251,,"If you make another commit could you include `[doc build]` in the commit message? This triggers a full build of the docs (the magic commit messages are explained in https://scikit-learn.org/dev/developers/contributing.html#commit-message-markers), which I think could be useful for reviewing how this look."
comment,33251,,"Thanks! I‚Äôve triggered a full docs build with [doc build].
I‚Äôll post before/after screenshots as soon as the preview is available.
"
pull_request,33250,DOC Fix broken link in california_housing.py,"Fixes #33231

The original link to the California Housing dataset (dcc.fc.up.pt) is down and returning a 504 Gateway Timeout.
This PR replaces the broken link in the comments with a working Wayback Machine archive link, as verified by manual download."
pull_request,33249,Fix float32 cond linear regression,"Fixes #33032 but will reopen #26164. 

### What does this implement/fix? Explain your changes.

As explained in #33032, the `cond` choice  in PR #30040 to solve #26164 introduced new bugs for instance on float32 data. This PR reverts the changes made in #30040 and adds a reproducer for the bugs reported in #33032.

### Comments / next steps

This a temporary ""fix"" following the plan outlined [here](https://github.com/scikit-learn/scikit-learn/issues/33032#issuecomment-3771756264). We need to reopen #26164.

In follow-up PRs we should either:
1. find a ""good"" choice for `cond` that ideally work on any data shape, dtype, and passes the sample weight consistency checks
2. expose `cond` as a parameter in `LinearRegression` as done for the sparse case in #30521, we could actually re-use the `tol` parameter for this (which would mean `cond` in the dense case)."
pull_request,33248,:lock: :robot: CI Update lock files for main CI build(s) :lock: :robot:,"Update lock files.

### Note
If the CI tasks fail, create a new branch based on this PR and add the required fixes to that branch."
pull_request,33247,:lock: :robot: CI Update lock files for array-api CI build(s) :lock: :robot:,"Update lock files.

### Note
If the CI tasks fail, create a new branch based on this PR and add the required fixes to that branch."
pull_request,33246,:lock: :robot: CI Update lock files for scipy-dev CI build(s) :lock: :robot:,"Update lock files.

### Note
If the CI tasks fail, create a new branch based on this PR and add the required fixes to that branch."
pull_request,33245,:lock: :robot: CI Update lock files for free-threaded CI build(s) :lock: :robot:,"Update lock files.

### Note
If the CI tasks fail, create a new branch based on this PR and add the required fixes to that branch."
issue,33244,"‚ö†Ô∏è CI failed on Unit tests Linux x86-64 pylatest_conda_forge_mkl (last failure: Feb 09, 2026) ‚ö†Ô∏è","**CI is still failing on [Unit tests Linux x86-64 pylatest_conda_forge_mkl](https://github.com/scikit-learn/scikit-learn/actions/runs/21811380974/job/62961607520)** (Feb 09, 2026)
- Test Collection Failure"
comment,33244,,"This is an `AssertionError` that I could not re-create locally with an environment made from the lockfile:

```py
>                   assert consensus_score(model.biclusters_, (rows, cols)) == 1
E                   assert 0.40740740740740744 == 1
 ../sklearn/cluster/tests/test_bicluster.py:130: AssertionError
```

`model.biclusters_` is a property method of `SpectralBiclustering` that returns wrong results in the CI.

I have manually triggered a re-run of the failing job. Let's see."
comment,33244,,"## CI is no longer failing! ‚úÖ

[Successful run](https://github.com/scikit-learn/scikit-learn/actions/runs/21850684719/job/63056471070) on Feb 10, 2026"
comment,33244,,"After rerunning the job, it is no longer failing.

I did not find any similar failures in the past. The only traceback related to `SpectralBiclustering.biclusters_` leads to a single line of code:

```py
    @property
    def biclusters_(self):
        """"""Convenient way to get row and column indicators together.

        Returns the ``rows_`` and ``columns_`` members.
        """"""
        return self.rows_, self.columns_
```

Given the nature of this error, I would consider it an odd and rare occurrence. I would therefore close this issue unless it rises again in the future."
comment,33244,,No new failure tonight. I will close this.
pull_request,33243,Update install.rst macOS instructions,"#### What does this implement/fix? Explain your changes.
Updated the install instructions to be more explicit, python doesn't always point to python3 on macOS

#### AI usage disclosure
<!--
If AI tools were involved in creating this PR, please check all boxes that apply
below and make sure that you adhere to our Automated Contributions Policy:
https://scikit-learn.org/dev/developers/contributing.html#automated-contributions-policy
-->
I used AI assistance for:
- [ ] Code generation (e.g., when writing an implementation or fixing a bug)
- [ ] Test/benchmark generation
- [ ] Documentation (including examples)
- [ ] Research and understanding


#### Any other comments?

"
issue,33242,The eigsh call in spectral embedding is done wrong and differently from what is described in the comments,"### Describe the bug and give evidence about its user-facing impact

The way `eigsh` is run in the spectral embedding routine is suboptimal and not consistent with what is described in the comments there.

Here are the relevant bits from https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/manifold/_spectral_embedding.py#L346

```python
        # Because the normalized Laplacian has eigenvalues between 0 and 2,
        # I - L has eigenvalues between -1 and 1.  ARPACK is most efficient
        ...
        # instead, we'll use ARPACK's shift-invert mode, asking for the
        # eigenvalues near 1.0.
        ...
            laplacian *= -1
            ...
            _, diffusion_map = eigsh(
                laplacian, k=n_components, sigma=1.0, which=""LM"", tol=tol, v0=v0
            )
```
The comments say that `eigsh` is supposed to be run on `I - L` and the eigenvalues of interest are close to 1, which is why `sigma=1.0` is used. This makes sense. However, in the code, the eigsh is run on `-L`. Of course it has the same eigenvectors, but the relevant eigenvalues are close to 0. So setting `sigma=1.0` does not make any sense.

To make the code follow what is described in the comments, one would need to add ones to the diagonal of `laplacian` prior to running `eigsh`.

The user-facing impact is that the runtime is slower than it could be (see below).

### Steps/Code to Reproduce

```python
import numpy as np
import pylab as plt
from scipy.sparse.linalg import eigsh
from scipy.sparse.csgraph import laplacian
from sklearn.utils._arpack import _init_arpack_v0
from sklearn.neighbors import kneighbors_graph
from sklearn.datasets import make_swiss_roll

X, _ = make_swiss_roll(n_samples=5000)

# These is identical to SpectralEmbedding(n_neighbors=10, eigen_solver=""arpack"", affinity=""nearest_neighbors"")
A = kneighbors_graph(X, n_neighbors=10, include_self=True).toarray()
A = (A + A.T) / 2

Lsym, dd = laplacian(A, normed=True, return_diag=True)
v0 = _init_arpack_v0(A.shape[0], 42)

np.fill_diagonal(Lsym, 1)
Lsym *= -1
# The next line is not present in the spectral embedding routine but it should be!
np.fill_diagonal(Lsym, 0)
    
%time eigvals, eigvecs = eigsh(Lsym, k=3, sigma=1, which=""LM"", tol=0, v0=v0)
eigvecs = eigvecs[:, ::-1]
Z = np.diag(1/dd) @ eigvecs
```

### Expected Results

Runtime 1.5 sec on my machine and the eiganvalues `[0.99935026, 0.99984568, 1.        ]`.

### Actual Results

Runtime 10.1 sec on my machine and eigenvalues `array([-6.55448604e-04, -1.62765707e-04, -1.99840144e-15])`.

### Versions

```shell
System:
    python: 3.14.2 | packaged by conda-forge | (main, Dec  6 2025, 11:21:58) [GCC 14.3.0]
executable: /home/dmitry/anaconda3/envs/nmds/bin/python
   machine: Linux-5.15.0-139-generic-x86_64-with-glibc2.31

Python dependencies:
      sklearn: 1.8.0
          pip: 25.3
   setuptools: 80.10.1
        numpy: 2.3.5
        scipy: 1.16.3
       Cython: None
       pandas: 2.3.3
   matplotlib: 3.10.8
       joblib: 1.5.3
threadpoolctl: 3.6.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libopenblas
       filepath: /home/dmitry/anaconda3/envs/nmds/lib/libopenblasp-r0.3.30.so
        version: 0.3.30
threading_layer: pthreads
   architecture: Haswell

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: libgomp
       filepath: /home/dmitry/anaconda3/envs/nmds/lib/libgomp.so.1.0.0
        version: None
```

### Interest in fixing the bug

One-line fix."
comment,33242,,"Nice catch! Could you please open a PR with the fix, and make sure you test comparing the PR with `main`? Should be an easy one to merge if I understand your comments correctly."
comment,33242,,"@adrinjalali I opened a minimal PR (#33262) but unexpectedly some tests are failing in some environments (while all passing on my machine), and at the moment I have no idea what may case it or how to debug it :-("
pull_request,33241,DOC: Fix double space formatting in related_projects.rst,"#### What does this implement/fix? Explain your changes.
Remove extra space between link markup and description text for Scikit-Learn Laboratory entry to maintain consistency with other project listings.

#### AI usage disclosure
<!--
If AI tools were involved in creating this PR, please check all boxes that apply
below and make sure that you adhere to our Automated Contributions Policy:
https://scikit-learn.org/dev/developers/contributing.html#automated-contributions-policy
-->
I used AI assistance for:
- [ ] Code generation (e.g., when writing an implementation or fixing a bug)
- [ ] Test/benchmark generation
- [ ] Documentation (including examples)
- [ ] Research and understanding


#### Any other comments?
Trying to get a feel of what it's like contributing to scikit-learn

"
comment,33241,,"> I'll accept this, but extras space are not rendered in rst thus we would generally not accept such PRs.

Oh, thanks :). I wanted to start with an easy task to familiarize myself with the codebase. Are there any first issues I could work on to learn the codebase a bit more? @lucyleeow "
pull_request,33240,MNT remove leftover authors,"#### Reference Issues/PRs
Follow-up of PR #29477 / issue #20813.

#### What does this implement/fix? Explain your changes.
This PR removes some leftover author names.

#### AI usage disclosure
No.

#### Any other comments?
Remaining are:
- `sklearn.utils.optimize.py` (kind of a license redistribution notice)"
issue,33239,[CODE CONTRIBUTION] Add incremental learning utilities,"### Describe the workflow you want to enable

Add IncrementalLearningWrapper to enable partial_fit usage with automatic batch management.

### Describe your proposed solution

Wrapper for partial_fit with streaming data support

### Describe alternatives you've considered, if relevant

_No response_

### Additional context

_No response_"
comment,33239,,See https://github.com/scikit-learn/scikit-learn/issues/33233#issuecomment-3868521182.
issue,33238,[CODE CONTRIBUTION] Add class imbalance metrics and tools,"### Describe the workflow you want to enable

Add specialized metrics for imbalanced datasets including macro and weighted variants.

### Describe your proposed solution

Extend metrics module with imbalance-aware scoring

### Describe alternatives you've considered, if relevant

_No response_

### Additional context

_No response_"
comment,33238,,See https://github.com/scikit-learn/scikit-learn/issues/33233#issuecomment-3868521182.
issue,33237,[CODE CONTRIBUTION] Add model performance degradation detector,"### Describe the workflow you want to enable

Add PerformanceDegradationDetector to monitor and alert when model performance decreases significantly.

### Describe your proposed solution

Use statistical tests to detect performance changes

### Describe alternatives you've considered, if relevant

_No response_

### Additional context

_No response_"
comment,33237,,See https://github.com/scikit-learn/scikit-learn/issues/33233#issuecomment-3868521182.
issue,33236,[CODE CONTRIBUTION] Add feature correlation detector,"### Describe the workflow you want to enable

Add WeightedVotingClassifier with learnable weights optimized during fit. Include weight learning via gradient descent.Add FeatureCorrelationDetector to identify and remove highly correlated features automatically.

### Describe your proposed solution

Use correlation matrix analysis to identify and remove redundant features

### Describe alternatives you've considered, if relevant

_No response_

### Additional context

_No response_"
comment,33236,,See https://github.com/scikit-learn/scikit-learn/issues/33233#issuecomment-3868521182.
issue,33235,[CODE CONTRIBUTION] Add robustness metrics for classifier evaluation,"### Describe the workflow you want to enable

Add adaptive learning rate schedulers for SGDClassifier/Regressor. Implement PolynomialDecay and ExponentialDecay schedulers with configurable parameAdd metrics to evaluate classifier robustness to adversarial perturbations and noise. Include RobustnessScore and AdversarialAccuracy metrics.
```python
class RobustnessScore:
    def __call__(self, y_true, y_pred, X, perturbation_strength=0.1):
        # Evaluate robustness
        pass
```ters.

### Describe your proposed solution

Use in SGDClassifier fit method for learning rate scheduAdd to sklearn.metrics moduleling

### Describe alternatives you've considered, if relevant

_No response_

### Additional context

_No response_"
comment,33235,,See https://github.com/scikit-learn/scikit-learn/issues/33233#issuecomment-3868521182.
issue,33234,[CODE CONTRIBUTION] Add model explainability metrics module,"### Describe the workflow you want to enable

Add explainability metrics to understand feature contributions using SHAP-like approach in sklearn.inspection. Include LocalExplainability and FeatureContribution classes.

```python
class LocalExplainability:
    def explain_prediction(self, X, y_pred):
        # Calculate contribution of each feature
        pass
```

### Describe your proposed solution

Implement as part of sklearn.inspection module.

### Describe alternatives you've considered, if relevant

_No response_

### Additional context

_No response_"
comment,33234,,See https://github.com/scikit-learn/scikit-learn/issues/33233#issuecomment-3868521182.
issue,33233,[CODE CONTRIBUTION] Add hyperparameter importance analysis utility,"### Describe the workflow you want to enable

## Proposed Feature

Add a utility function to analyze feature importance across different hyperparameter values, helping users understand which hyperparameters have the most impact on model performance.

## Proposed Implementation

```python
# Add to sklearn.model_selection

class HyperparameterImportance:
    def __init__(self, estimator, param_grid, cv=5):
        self.estimator = estimator
        self.param_grid = param_grid
        self.cv = cv
        self.importance_scores_ = None
    
    def fit(self, X, y):
        """"""Calculate hyperparameter importance.""""""
        from itertools import product
        param_names = list(self.param_grid.keys())
        param_values = [self.param_grid[name] for name in param_names]
        
        results = []
        for param_combo in product(*param_values):
            params = dict(zip(param_names, param_combo))
            self.estimator.set_params(**params)
            
            scores = cross_val_score(self.estimator, X, y, cv=self.cv)
            mean_score = scores.mean()
            results.append((params, mean_score))
        
        # Calculate importance as variance contribution
        self.importance_scores_ = self._calculate_importance(results)
        return self
    
    def _calculate_importance(self, results):
        """"""Calculate importance scores for each hyperparameter.""""""
        import numpy as np
        param_names = list(self.param_grid.keys())
        importance = {name: 0 for name in param_names}
        
        for i, param_name in enumerate(param_names):
            values = []
            for params, score in results:
                values.append(score)
            importance[param_name] = np.var(values)
        
        return importance
```

## Benefits
- Helps identify most impactful hyperparameters
- Guides hyperparameter tuning efforts
- Improves computational efficiency
- Provides insights for model optimization

### Describe your proposed solution

The solution is outlined in the implementation code above. Users would instantiate HyperparameterImportance with their estimator and parameter grid, fit it to their data, and access importance_scores_ to understand which hyperparameters have the most impact on model performance.

### Describe alternatives you've considered, if relevant

_No response_

### Additional context

_No response_"
comment,33233,,"Yeah u can

On Sat, 7 Feb 2026 at 23:13, Seyi Kuforiji ***@***.***> wrote:

> *Seyi007* left a comment (scikit-learn/scikit-learn#33233)
> <https://github.com/scikit-learn/scikit-learn/issues/33233#issuecomment-3864926812>
>
> Hi @swamy18 <https://github.com/swamy18>, can I work on this issue?
>
> ‚Äî
> Reply to this email directly, view it on GitHub
> <https://github.com/scikit-learn/scikit-learn/issues/33233#issuecomment-3864926812>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/A5H3Y26OYA5QCYPEVUKQEET4KYP27AVCNFSM6AAAAACUKPNGK6VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZTQNRUHEZDMOBRGI>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
"
comment,33233,,"Thanks for opening the issue. 

This appears to be spam. [Similar content has been posted across multiple repos](https://github.com/search?q=involves%3Aswamy18+&type=issues). 

I'm closing it here to keep the tracker clean."
pull_request,33232,Adjusted the line spacing,"

#### What does this implement/fix? Explain your changes.
adjust the line spacing

I used AI assistance for:
- [ ] Code generation (e.g., when writing an implementation or fixing a bug)
- [ ] Test/benchmark generation
- [ ] Documentation (including examples)
- [x] Research and understanding
"
issue,33231,Callifornia housing dataset link is not available,"### Describe the bug and give evidence about its user-facing impact

The link: https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.tgz redirects to 504 Gateway Time-out.

### Steps/Code to Reproduce

Dataset link issue.

### Expected Results

The link might get changed.

### Actual Results

504 Gateway Time-out


### Versions

```shell
System:
    python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]
executable: /usr/bin/python3
   machine: Linux-6.6.105+-x86_64-with-glibc2.35

Python dependencies:
      sklearn: 1.6.1
          pip: 24.1.2
   setuptools: 75.2.0
        numpy: 2.0.2
        scipy: 1.16.3
       Cython: 3.0.12
       pandas: 2.2.2
   matplotlib: 3.10.0
       joblib: 1.5.3
threadpoolctl: 3.6.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 2
         prefix: libscipy_openblas
       filepath: /usr/local/lib/python3.12/dist-packages/numpy.libs/libscipy_openblas64_-99b71e71.so
        version: 0.3.27
threading_layer: pthreads
   architecture: Haswell

       user_api: blas
   internal_api: openblas
    num_threads: 2
         prefix: libscipy_openblas
       filepath: /usr/local/lib/python3.12/dist-packages/scipy.libs/libscipy_openblas-b75cc656.so
        version: 0.3.29.dev
threading_layer: pthreads
   architecture: Haswell

       user_api: openmp
   internal_api: openmp
    num_threads: 2
         prefix: libgomp
       filepath: /usr/local/lib/python3.12/dist-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None
```

### Interest in fixing the bug

Please correct the link in the repo."
comment,33231,,"Can confirm link does not work.

Note it is only mentioned in a comment:

https://github.com/scikit-learn/scikit-learn/blob/a9476907765503bee41f74d030ea090424c40352/sklearn/datasets/_california_housing.py#L45-L46"
issue,33230,permutation_importance ‚Üí pkg_resources is deprecated as an API,"### Describe the bug and give evidence about its user-facing impact

The permutation_importance() in combination with import hyperopt causes UserWarning about deprecated pkg_resources from multiprocessing with reference of removal as early as 2025-11-30 as example below shows. 

### Steps/Code to Reproduce

```
import numpy as np
from sklearn.linear_model import LogisticRegression as LR
from sklearn.inspection import permutation_importance
import hyperopt  # ‚óÑ with this import the warning is triggered 

s = 100
X = np.random.rand(s,10)
y = np.random.randint(0, 2, size=(s, 1)).ravel()
m = LR().fit(X,y)

permutation_importance(
            m, X, y, 
            n_repeats=10, 
            random_state=42, 
            scoring='roc_auc',
            n_jobs=-1
)
```

### Expected Results

no UserWarning regardless other imports


### Actual Results

/usr/lib/python3.12/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  return _ForkingPickler.loads(res)
/usr/lib/python3.12/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  return _ForkingPickler.loads(res)
/usr/lib/python3.12/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  return _ForkingPickler.loads(res)
/usr/lib/python3.12/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  return _ForkingPickler.loads(res)
/usr/lib/python3.12/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  return _ForkingPickler.loads(res)
/usr/lib/python3.12/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  return _ForkingPickler.loads(res)
/usr/lib/python3.12/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  return _ForkingPickler.loads(res)
/usr/lib/python3.12/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  return _ForkingPickler.loads(res)
/usr/lib/python3.12/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  return _ForkingPickler.loads(res)
/usr/lib/python3.12/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  return _ForkingPickler.loads(res)

### Versions

```shell
System:
    python: 3.12.3 (main, Jan 22 2026, 20:57:42) [GCC 13.3.0]
executable: /x-venv-3.12/bin/python3
   machine: Linux-6.17.0-14-generic-x86_64-with-glibc2.39

Python dependencies:
      sklearn: 1.8.0
          pip: 24.0
   setuptools: 80.9.0
        numpy: 2.3.5
        scipy: 1.15.3
       Cython: None
       pandas: 3.0.0
   matplotlib: 3.10.8
       joblib: 1.5.1
threadpoolctl: 3.6.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 32
         prefix: libscipy_openblas
       filepath: /x-venv-3.12/lib/python3.12/site-packages/numpy.libs/libscipy_openblas64_-fdde5778.so
        version: 0.3.30
threading_layer: pthreads
   architecture: SkylakeX

       user_api: blas
   internal_api: openblas
    num_threads: 32
         prefix: libscipy_openblas
       filepath: /x-venv-3.12/lib/python3.12/site-packages/scipy.libs/libscipy_openblas-68440149.so
        version: 0.3.28
threading_layer: pthreads
   architecture: SkylakeX

       user_api: openmp
   internal_api: openmp
    num_threads: 32
         prefix: libgomp
       filepath: /x-venv-3.12/lib/python3.12/site-packages/scikit_learn.libs/libgomp-e985bcbb.so.1.0.0
        version: None

       user_api: openmp
   internal_api: openmp
    num_threads: 32
         prefix: libgomp
       filepath: /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0
        version: None
```

### Interest in fixing the bug

?"
comment,33230,,"A straightforward and low-risk way to address this would be to deduplicate the warning and ensure it is emitted only once.

The deprecation warning itself is valid and originates from a third-party dependency, so suppressing it entirely would not be appropriate. However, repeatedly emitting the same warning during multiprocessing result collection does not add additional value and can be confusing for users.

Emitting a single, contextualized warning would preserve transparency about the underlying issue while avoiding excessive noise, without affecting results or scikit-learn‚Äôs public API."
comment,33230,,"Hi , I did local reproduce and local fix validation for this issue.

### 1. Version info


#### 1.1 Reproduce env

```text
Python 3.12.12
python_executable: /Users/gufang/Desktop/Uni/2st semester/Software Simulation Engineering/challenge/scikit-learn/.venv-33230/bin/python
sklearn: 1.8.0
hyperopt: 0.2.7
setuptools: 80.9.0
numpy: 2.4.2
scipy: 1.17.0
joblib: 1.5.3
threadpoolctl: 3.6.0
```

#### 1.2 Fix env

```text
Python 3.12.12
python_executable: /Users/gufang/Desktop/Uni/2st semester/Software Simulation Engineering/challenge/scikit-learn/.venv-dev/bin/python
sklearn: 1.9.dev0
hyperopt: 0.2.7
setuptools: 80.9.0
numpy: 2.4.2
scipy: 1.17.0
joblib: 1.5.3
threadpoolctl: 3.6.0
```

### 2. Local machine info

Output:

```text
Darwin Mac.zyxel.box 24.6.0 Darwin Kernel Version 24.6.0: Wed Nov  5 21:34:00 PST 2025; root:xnu-11417.140.69.705.2~1/RELEASE_ARM64_T8132 arm64
```

### 3. Reproduce result

I used the issue script (same logic: `permutation_importance(..., n_jobs=-1)` with `import hyperopt`) and saved output.

```text
/Users/gufang/Desktop/Uni/2st semester/Software Simulation Engineering/challenge/scikit-learn/.venv-33230/lib/python3.12/site-packages/hyperopt/atpe.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
Running permutation_importance...
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  return _ForkingPickler.loads(res)
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  return _ForkingPickler.loads(res)
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  return _ForkingPickler.loads(res)
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  return _ForkingPickler.loads(res)
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  return _ForkingPickler.loads(res)
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  return _ForkingPickler.loads(res)
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  return _ForkingPickler.loads(res)
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  return _ForkingPickler.loads(res)
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  return _ForkingPickler.loads(res)
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  return _ForkingPickler.loads(res)
Done. Mean importances: [0.04700289 0.01310459 0.0124845 ]
```

Count check:

```text
10
11
```

### 4. Root cause analysis

This repeated warning is not from `permutation_importance` algorithm itself.

Root cause is in `sklearn.utils.parallel` warning filter propagation:

- scikit-learn sends `warnings.filters` from main process to worker process
- some filter categories are third-party warning classes (example from `pkg_resources`)
- worker unpickle of those class objects can import third-party module again
- import side-effect triggers same deprecation warning many times

So users see repeated warning spam from multiprocessing queue loading.

### 5. Fix solution

I implemented minimal change in `sklearn/utils/parallel.py`:

- before send filters to workers:
  - replace category class object by `(module, qualname)`
- in worker restore stage:
  - resolve category only from `sys.modules` (already-loaded modules)
  - if not loaded / not found, skip this filter

This avoids implicit third-party import at filter restore stage.

### 6. Result after fix

```text
/Users/gufang/Desktop/Uni/2st semester/Software Simulation Engineering/challenge/scikit-learn/.venv-dev/lib/python3.12/site-packages/hyperopt/atpe.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
sklearn 1.9.dev0
ok [0.00024351 0.00316558 0.00174513]
```

Count check:

```text
0
1
```

### 7. Test validation

```text
.......ss.........                                                       [100%]
16 passed, 2 skipped in 6.89s

........ssss.....s...ssssss.s......                                      [100%]
23 passed, 12 skipped in 2.83s
```

### 8. Request

I will open a PR for this

### 9. Attention
Attention: AI was used to correct English grammar, vocabulary, and sentence structure.
"
pull_request,33229,Use minlength=n_bins instead of len(bins) for clarity and futureproofing,"<!--
üôå Thanks for contributing a pull request!

üëÄ Please ensure you have taken a look at the contribution guidelines:
https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md

‚úÖ In particular following the pull request checklist will increase the likelihood
of having maintainers review your PR:
https://scikit-learn.org/dev/developers/contributing.html#pull-request-checklist

üìã If your PR is likely to affect users, you will need to add a changelog entry
describing your PR changes, see:
https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/README.md
-->

#### Reference Issues/PRs
<!--
Fixes #18130 
-->
Fixes #18130 

#### What does this implement/fix? Explain your changes.

Changed the minlength argument in np.bincount calls from len(bins) to n_bins in sklearn/calibration.py. This improves readability and prevents potential future bugs if the number of bins and the bins array length diverge without being handled. Not expected to change any behaviour in present version. Verified with pytest on calibration.py anyway; same results as before changes.

#### AI usage disclosure
<!--
If AI tools were involved in creating this PR, please check all boxes that apply
below and make sure that you adhere to our Automated Contributions Policy:
https://scikit-learn.org/dev/developers/contributing.html#automated-contributions-policy
-->
I used AI assistance for:
- [ ] Code generation (e.g., when writing an implementation or fixing a bug)
- [ ] Test/benchmark generation
- [ ] Documentation (including examples)
- [x] Research and understanding


#### Any other comments?
AI assistance checked above was for handling dependency conflicts on my stupid machine.

<!--
Thank you for your patience. Changes to scikit-learn require careful
attention, but with limited maintainer time, not every contribution can be reviewed
quickly.
For more information and tips on improving your pull request, see:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
"
pull_request,33228,Adjusted the spacing and line breaks slightly,"
#### What does this implement/fix? Explain your changes.
I just adjusted the line spacing of the file keeping everything else the same

#### AI usage disclosure
<!--
If AI tools were involved in creating this PR, please check all boxes that apply
below and make sure that you adhere to our Automated Contributions Policy:
https://scikit-learn.org/dev/developers/contributing.html#automated-contributions-policy
-->
I used AI assistance for:
- [ ] Code generation (e.g., when writing an implementation or fixing a bug)
- [ ] Test/benchmark generation
- [ ] Documentation (including examples)
- [x] Research and understanding





"
comment,33228,,"## ‚ùå Linting issues

This PR is introducing linting issues. Here's a summary of the issues. Note that you can avoid having linting issues by enabling `pre-commit` hooks. Instructions to enable them can be found [here](https://scikit-learn.org/dev/developers/development_setup.html#set-up-pre-commit).

You can see the details of the linting issues under the `lint` job [here](https://github.com/scikit-learn/scikit-learn/actions/runs/21755794435)

-----------------------------------------------
### `ruff check`

`ruff` detected issues. Please run `ruff check --fix --output-format=full` locally, fix the remaining issues, and push the changes. Here you can see the detected issues. Note that the installed `ruff` version is `ruff=0.12.2`.

<details>

```

.github/scripts/label_title_regex.py:4:1: I001 [*] Import block is un-sorted or un-formatted
  |
2 |   pull_request_target event.""""""
3 |
4 | / import json
5 | | import os
6 | | import re
7 | | from github import Github
  | |_________________________^ I001
8 |
9 |   context_dict = json.loads(os.getenv(""CONTEXT_GITHUB""))
  |
  = help: Organize imports

Found 1 error.
[*] 1 fixable with the `--fix` option.
```

</details>



<sub> _Generated for commit: [2dfabbf](https://github.com/scikit-learn/scikit-learn/pull/33228/commits/2dfabbf345d15d9923143bfa4bc2e5aa0c458354). Link to the linter CI: [here](https://github.com/scikit-learn/scikit-learn/actions/runs/21755794435)_ </sub>"
issue,33227,Segmentation fault with free-threaded in `test_gpr_correct_error_message`,"Noticed in https://github.com/scikit-learn/scikit-learn/pull/33221 see [build log](https://github.com/scikit-learn/scikit-learn/actions/runs/21751910937/job/62751865357?pr=33221)

```
gaussian_process/tests/test_gpc.py ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑ [ 15%]
¬∑¬∑¬∑¬∑¬∑¬∑                                                                   [ 15%]
gaussian_process/tests/test_gpr.py ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑ [ 15%]
Fatal Python error: Segmentation fault

<Cannot show all threads while the GIL is disabled>
Stack (most recent call first):
  File ""/home/runner/work/scikit-learn/scikit-learn/sklearn/gaussian_process/kernels.py"", line 280 in hyperparameters
  File ""/home/runner/work/scikit-learn/scikit-learn/sklearn/gaussian_process/kernels.py"", line 301 in theta
  File ""/home/runner/work/scikit-learn/scikit-learn/sklearn/gaussian_process/kernels.py"", line 273 in n_dims
  File ""/home/runner/work/scikit-learn/scikit-learn/sklearn/gaussian_process/_gpr.py"", line 302 in fit
  File ""/home/runner/work/scikit-learn/scikit-learn/sklearn/base.py"", line 1347 in wrapper
  File ""/home/runner/work/scikit-learn/scikit-learn/sklearn/gaussian_process/tests/test_gpr.py"", line 430 in test_gpr_correct_error_message
  File ""/home/runner/miniconda3/envs/testvenv/lib/python3.14t/site-packages/pytest_run_parallel/plugin.py"", line 80 in closure
  File ""/home/runner/miniconda3/envs/testvenv/lib/python3.14t/threading.py"", line 1024 in run
  File ""/home/runner/miniconda3/envs/testvenv/lib/python3.14t/threading.py"", line 1082 in _bootstrap_inner
  File ""/home/runner/miniconda3/envs/testvenv/lib/python3.14t/threading.py"", line 1044 in _bootstrap

Current thread's C stack trace (most recent call first):
  Binary file ""python"", at _Py_DumpStack+0x4a [0x55af10bf2982]
  Binary file ""python"", at +0x16a1a8 [0x55af10bfd1a8]
  Binary file ""/lib/x86_64-linux-gnu/libc.so.6"", at +0x45330 [0x7f8384445330]
  Binary file ""python"", at +0x18b5a0 [0x55af10c1e5a0]
  Binary file ""python"", at +0x1921b0 [0x55af10c251b0]
  Binary file ""python"", at +0x1907bb [0x55af10c237bb]
  Binary file ""python"", at +0x19e926 [0x55af10c31926]
  Binary file ""python"", at +0x2e84fc [0x55af10d7b4fc]
  Binary file ""python"", at +0x2e857f [0x55af10d7b57f]
  Binary file ""python"", at +0x2e7c61 [0x55af10d7ac61]
  Binary file ""python"", at +0x19b967 [0x55af10c2e967]
  Binary file ""python"", at PyObject_Dir+0x44 [0x55af10d57354]
  Binary file ""python"", at +0x2c42e6 [0x55af10d572e6]
  Binary file ""python"", at +0x1e21d9 [0x55af10c751d9]
  Binary file ""python"", at _PyObject_MakeTpCall+0x3fb [0x55af10c3347b]
  Binary file ""python"", at _PyEval_EvalFrameDefault+0x14e1 [0x55af10c4cba1]
  Binary file ""python"", at +0x1ef6db [0x55af10c826db]
  Binary file ""python"", at PyObject_CallOneArg+0x58 [0x55af10c8a498]
  Binary file ""python"", at _PyObject_GenericGetAttrWithDict+0x32d [0x55af10c739cd]
  Binary file ""python"", at PyObject_GetAttr+0x44 [0x55af10c37004]
  Binary file ""python"", at _PyEval_EvalFrameDefault+0x1133 [0x55af10c4c7f3]
  Binary file ""python"", at +0x1ef6db [0x55af10c826db]
  Binary file ""python"", at PyObject_CallOneArg+0x58 [0x55af10c8a498]
  Binary file ""python"", at _PyObject_GenericGetAttrWithDict+0x32d [0x55af10c739cd]
  Binary file ""python"", at PyObject_GetAttr+0x44 [0x55af10c37004]
  Binary file ""python"", at _PyEval_EvalFrameDefault+0x1133 [0x55af10c4c7f3]
  Binary file ""python"", at +0x1ef6db [0x55af10c826db]
  Binary file ""python"", at PyObject_CallOneArg+0x58 [0x55af10c8a498]
  Binary file ""python"", at _PyObject_GenericGetAttrWithDict+0x32d [0x55af10c739cd]
  Binary file ""python"", at PyObject_GetAttr+0x44 [0x55af10c37004]
  Binary file ""python"", at _PyEval_EvalFrameDefault+0x1133 [0x55af10c4c7f3]
  Binary file ""python"", at +0x1b5100 [0x55af10c48100]
  <truncated rest of calls>

Extension modules: numpy._core._multiarray_umath, numpy.linalg._umath_linalg, sklearn.__check_build._check_build, cython.cimports.libc.math, _cyutility, scipy._cyutility, scipy._lib._ccallback_c, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._pcg64, numpy.random._generator, numpy.random._mt19937, numpy.random._philox, numpy.random._sfc64, numpy.random.mtrand, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._batched_linalg, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpacklib, scipy.sparse.linalg._propack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation_cy, scipy.spatial.transform._rigid_transform_cy, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, scipy.io.matlab._mio_utils, scipy.io.matlab._streams, scipy.io.matlab._mio5_utils, sklearn.datasets._svmlight_format_fast, sklearn.utils._random, sklearn.utils._vector_sentinel, sklearn.feature_extraction._hashing_fast, _loss, sklearn._loss._loss, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, sklearn.utils._fast_dict, sklearn.cluster._hierarchical_fast, sklearn.cluster._k_means_common, sklearn.cluster._k_means_elkan, sklearn.cluster._k_means_lloyd, sklearn.cluster._k_means_minibatch, sklearn.cluster._dbscan_inner, sklearn.neighbors._partition_nodes, sklearn.neighbors._ball_tree, sklearn.neighbors._kd_tree, sklearn.utils.arrayfuncs, sklearn.utils._seq_dataset, sklearn.linear_model._cd_fast, sklearn.linear_model._sag_fast, sklearn.svm._liblinear, sklearn.svm._libsvm, sklearn.svm._libsvm_sparse, sklearn.utils._weight_vector, sklearn.linear_model._sgd_fast, sklearn.decomposition._online_lda_fast, sklearn.decomposition._cdnmf_fast, sklearn.cluster._hdbscan._tree, sklearn.cluster._hdbscan._linkage, sklearn.cluster._hdbscan._reachability, sklearn._isotonic, sklearn.tree._utils, sklearn.tree._tree, sklearn.tree._partitioner, sklearn.tree._splitter, sklearn.tree._criterion, sklearn.neighbors._quad_tree, sklearn.manifold._barnes_hut_tsne, sklearn.manifold._utils, scipy.cluster._vq, scipy.cluster._hierarchy, scipy.cluster._optimal_leaf_ordering, sklearn.ensemble._gradient_boosting, sklearn.ensemble._hist_gradient_boosting.common, sklearn.ensemble._hist_gradient_boosting._gradient_boosting, sklearn.ensemble._hist_gradient_boosting._binning, sklearn.ensemble._hist_gradient_boosting._bitset, sklearn.ensemble._hist_gradient_boosting.histogram, sklearn.ensemble._hist_gradient_boosting._predictor, sklearn.ensemble._hist_gradient_boosting.splitting, sklearn.svm._newrand, sklearn.utils._typedefs (total: 170)
build_tools/azure/test_script.sh: line 92:  3809 Segmentation fault      (core dumped) python -m pytest --showlocals --durations=20 --junitxml=test-data.xml -o junit_family=legacy --parallel-threads 4 --iterations 1 --pyargs sklearn
¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑
Error: Process completed with exit code 139.
```"
comment,33227,,"I can not reproduce it locally with the command below but since `hyperparameters` is in the stack-trace I am wondering whether this is a variant of https://github.com/scikit-learn/scikit-learn/issues/33152 (which I was not able to reproduce either) ü§î?
```
pytest sklearn/gaussian_process/tests/test_gpr.py --parallel-threads 4 --iterations 100
```

Edit: probably not a variation since there is no share state through parametrization contrary to #33152 ...
https://github.com/scikit-learn/scikit-learn/blob/bf223d9c21d3f85de90592f69b9a8710fecbc939/sklearn/gaussian_process/tests/test_gpr.py#L418-L430"
comment,33227,,"Hi! I‚Äôm looking into reproducing this on my side (free-threaded / --parallel-threads) and will report back with findings. If nobody is currently working on it, I‚Äôd like to take it.
"
pull_request,33226,DOC Improve Linear Regression documentation with real-world use case,"<!--
üôå Thanks for contributing a pull request!

üëÄ Please ensure you have taken a look at the contribution guidelines:
https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md

‚úÖ In particular following the pull request checklist will increase the likelihood
of having maintainers review your PR:
https://scikit-learn.org/dev/developers/contributing.html#pull-request-checklist

üìã If your PR is likely to affect users, you will need to add a changelog entry
describing your PR changes, see:
https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/README.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.


#### AI usage disclosure
<!--
If AI tools were involved in creating this PR, please check all boxes that apply
below and make sure that you adhere to our Automated Contributions Policy:
https://scikit-learn.org/dev/developers/contributing.html#automated-contributions-policy
-->
I used AI assistance for:
- [ ] Code generation (e.g., when writing an implementation or fixing a bug)
- [ ] Test/benchmark generation
- [ ] Documentation (including examples)
- [ ] Research and understanding


#### Any other comments?


<!--
Thank you for your patience. Changes to scikit-learn require careful
attention, but with limited maintainer time, not every contribution can be reviewed
quickly.
For more information and tips on improving your pull request, see:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
"
pull_request,33225,TST Use assert_allclose to compare floats,The test `test_estimator_get_response_values` was modified recently (e.g. [here](https://github.com/scikit-learn/scikit-learn/actions/runs/21750614826/job/62747443149?pr=33224)) and now fails sometimes because of numerical precision. `assert_allclose` should be used to compare floats
comment,33225,,"Thanks for the PR, see ongoing https://github.com/scikit-learn/scikit-learn/pull/33221 though :wink: "
comment,33225,,closing in favor of #33221 
pull_request,33224,"DOC Rephrase the param description of ""name"" in RocCurveDisplay","Related to https://github.com/scikit-learn/scikit-learn/pull/30508 and this https://github.com/scikit-learn/scikit-learn/pull/30508#discussion_r2772070275 in particular.

Better separate the 3 possibilities for the parameter and clearer description of how the para is used.

@lucyleeow what do you think ?"
comment,33224,,@jeremiedbb @ogrisel feel free to merge if you are happy.
pull_request,33223,TST Refactor out helper in `pos_label` display curve tests,"#### Reference Issues/PRs
Related to #33218 and https://github.com/scikit-learn/scikit-learn/pull/33217
Part of PRs to move test related changes out of #30508 and into separate PR


#### What does this implement/fix? Explain your changes.
Takes the common part of `test_plot_roc_curve_pos_label` (which is shared with `test_plot_precision_recall_pos_label`) out to a helper function in `test_common_curve_display.py`, to reduce duplication.

#### AI usage disclosure
<!--
If AI tools were involved in creating this PR, please check all boxes that apply
below and make sure that you adhere to our Automated Contributions Policy:
https://scikit-learn.org/dev/developers/contributing.html#automated-contributions-policy
-->
I used AI assistance for:
- [ ] Code generation (e.g., when writing an implementation or fixing a bug)
- [ ] Test/benchmark generation
- [ ] Documentation (including examples)
- [ ] Research and understanding


#### Any other comments?


cc @jeremiedbb @ogrisel :pray: 
"
comment,33223,,"I think you stopped writing the PR title halfway through :)

> which is shared with `test_plot_precision_recall_pos_label`

Shouldn't we make this test use the new helper as part of this PR then ?"
comment,33223,,"> Shouldn't we make this test use the new helper as part of this PR then ?

I avoided it for the merge conflicts as there's from_cv_result specific stuff.

E.g. Despite being careful somehow still have merge related problem, picked up in #33217"
comment,33223,,"> I avoided it for the merge conflicts as there's from_cv_result specific stuff.

Oh right, fine then"
comment,33223,,Thank you for your understanding :pray: 
comment,33223,,@lorentzenchr happy to merge?
comment,33223,,"Thanks for the review @lorentzenchr, sorry my responses may not have been satisfactory."
pull_request,33222,FIX: avoid quadratic path for constant arrays in `simultaneous_sort`,"#### Reference Issues/PRs

Fixes partially #33167

#### Context: quick sort variants & intro sort

In quick sort, you pick a pivot and then partition the array, but there are variants on how to partition the array:
- 2-way partition: partition the array in `[x < pivot] [x >= pivot]` which is what is currently implemented, or `[x <= pivot] [x >= pivot]` which is what numpy does
- 3-way partition: partition the array in `[x < pivot] [x == pivot] [x > pivot]`. This is very efficient when there are a lot of duplicate values, but otherwise it's less efficient than the 2-way scheme. 

I think that all those schemes can degenerate in O(n^2) complexity for ""adversarial"" inputs, that's why we usually ""guard"" them with a heap sort. This mix is called intro sort. For instance, decision trees implement an intro sort with a 3-way partition quick sort.

#### What does this implement/fix? Explain your changes.

For the current partitioning scheme used in `utils/_sorting.pyx` (`[x < pivot] [x >= pivot]`) the ""adversarial input"" is as simple as a constant array (or arrays with many duplicates), which is a very common case.

So I propose to simply change this to the numpy-style partitioning: `[x <= pivot] [x >= pivot]` which doesn't suffer from this problem. But it can still go quadratic for an input like this: `[1, 2, 3, ..., n-1, 0]` (much more unlikely than constant/many-duplicates arrays, but still not that crazy).

To be really safe, we should implement an intro sort, I'll create another PR to show what it looks like. Here it is: #33252

#### AI usage:

Used it to find the `[1, 2, 3, ..., n-1, 0]` pattern.

"
pull_request,33221,FIX add missing random states and cloning in `test_response.py`,"<!--
üôå Thanks for contributing a pull request!

üëÄ Please ensure you have taken a look at the contribution guidelines:
https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md

‚úÖ In particular following the pull request checklist will increase the likelihood
of having maintainers review your PR:
https://scikit-learn.org/dev/developers/contributing.html#pull-request-checklist

üìã If your PR is likely to affect users, you will need to add a changelog entry
describing your PR changes, see:
https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/README.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Towards #32393 and possibly also #33216

#### What does this implement/fix? Explain your changes.
I added the missing `random_seed` to the `KMeans` test cases and also added `clone` before `fit` (as it is done for example in line 240) where it was missing.

@lesteve 
#### AI usage disclosure
<!--
If AI tools were involved in creating this PR, please check all boxes that apply
below and make sure that you adhere to our Automated Contributions Policy:
https://scikit-learn.org/dev/developers/contributing.html#automated-contributions-policy
-->
I used AI assistance for:
- [ ] Code generation (e.g., when writing an implementation or fixing a bug)
- [ ] Test/benchmark generation
- [ ] Documentation (including examples)
- [ ] Research and understanding


#### Any other comments?


<!--
Thank you for your patience. Changes to scikit-learn require careful
attention, but with limited maintainer time, not every contribution can be reviewed
quickly.
For more information and tips on improving your pull request, see:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
"
comment,33221,,"Not sure why, but I am still getting a failure with free-threaded locally with the following command:
```
pytest sklearn/utils/tests/test_response.py --parallel-threads 4 --iterations 10
```

```
PARALLEL FAILED sklearn/utils/tests/test_response.py::test_estimator_get_response_values[True-estimator2-score] - AssertionError: 
PARALLEL FAILED sklearn/utils/tests/test_response.py::test_estimator_get_response_values[False-estimator2-score] - AssertionError: 
```

Details for one of the errors:
```
_________________________________________ ERROR at call of test_estimator_get_response_values[False-estimator2-score] _________________________________________

estimator = KMeans(n_clusters=2, random_state=0), response_method = 'score', return_response_method_used = False

    @pytest.mark.parametrize(
        ""estimator, response_method"",
        [
            (LinearRegression(), ""predict""),
            (KMeans(n_clusters=2, random_state=0), ""predict""),
            (KMeans(n_clusters=2, random_state=0), ""score""),
            (KMeans(n_clusters=2, random_state=0), [""predict"", ""score""]),
            (IsolationForest(random_state=0), ""predict""),
            (IsolationForest(random_state=0), ""decision_function""),
            (IsolationForest(random_state=0), [""decision_function"", ""predict""]),
        ],
    )
    @pytest.mark.parametrize(""return_response_method_used"", [True, False])
    def test_estimator_get_response_values(
        estimator, response_method, return_response_method_used
    ):
        """"""Check the behaviour of `_get_response_values`.""""""
        X, y = np.random.RandomState(0).randn(10, 2), np.array([0, 1] * 5)
        estimator = clone(estimator).fit(X, y)  # clone to make test execution thread-safe
        results = _get_response_values(
            estimator,
            X,
            response_method=response_method,
            return_response_method_used=return_response_method_used,
        )
        chosen_response_method = (
            response_method[0] if isinstance(response_method, list) else response_method
        )
        prediction_method = getattr(estimator, chosen_response_method)
>       assert_array_equal(results[0], prediction_method(X))
E       AssertionError: 
E       Arrays are not equal
E       
E       Mismatched elements: 1 / 1 (100%)
E       Max absolute difference among violations: 8.8817842e-16
E       Max relative difference among violations: 1.39609109e-16
E        ACTUAL: array(-6.361894)
E        DESIRED: array(-6.361894)

sklearn/utils/tests/test_response.py:81: AssertionError
```"
comment,33221,,"```
pytest sklearn/utils/tests/test_response.py --parallel-threads 4 --iterations 10
```
For me this passes locally, but the code on `main` actually does, too (on Linux) :shrug:"
comment,33221,,"Oh, hang on, I also get the following messages when setting `PYTEST_RUN_PARALLEL_VERBOSE=1`

>sklearn/utils/tests/test_response.py::test_estimator_get_response_values[False-estimator2-score] was not run in parallel because it calls thread-unsafe function: warnings.catch_warnings

Do you know how I can overwrite this to test it locally?
"
comment,33221,,">Not sure why, but I am still getting a failure with free-threaded locally

So `score` seems to be the culprit here. It does indeed return a float, so maybe this test doesn't make sense here? I'm not very familiar with the use cases for the different scoring functions yet, but the test for `LinearRegression` also only checks `predict`, so maybe we could just remove `score` here?"
comment,33221,,"Another option would be to see if it is enough to replace `assert_array_equal` with `assert_allclose` if `prediction_method=""score""`."
comment,33221,,">Do you know how I can overwrite this to test it locally?

I guess upgrading to Python 3.14 is required, right? I'll give that a try."
comment,33221,,"> I guess upgrading to Python 3.14 is required, right? I'll give that a try.

In general with free-threaded it's better to use Python 3.14 indeed. The warning you get does ring a bell and I think you will not get it if you use Python 3.14 and you will get the failure instead."
comment,33221,,"Actually, it does not seem to be related to pytest-run-parallel I do get occasional failures with free-threaded
```
pytest sklearn/utils/tests/test_response.py -k 'test_estimator_get_response_values and estimator2' 
```

With pytest-repeat you can run the test multiple times and it will fail at one point:
```
pytest sklearn/utils/tests/test_response.py -k 'test_estimator_get_response_values and estimator2' --count 10
```"
comment,33221,,"To be honest, using `assert_allclose` is probably the right work-around to use, because this is the thing we do to compare floating points ...

The relative difference is close to 1e-16 and so it is close to the float64 resolution, and I don't think we should spend too much time on this. Why it only happens for free-threaded, I don't know, there may be something fishy there, hard to tell ..."
comment,33221,,This is not free-threaded specific I do see a similar thing as https://github.com/scikit-learn/scikit-learn/pull/33221#issuecomment-3860038509 with vanilla Python 3.14.
comment,33221,,Should we still keep the random state and cloning changes or just merge https://github.com/scikit-learn/scikit-learn/pull/33225 instead?
comment,33221,,"Let's keep this one I think, because I think it also fixes free-threaded problems."
issue,33220,ENH partial_dependence should not require to inherit from BaseEstimator,"### Current status
`partial_dependence` requires that an estimator has `__sklearn_tags__` which basically requires that it inherits from `BaseEstimator`.
```py
import numpy as np
from sklearn.inspection import partial_dependence

X, y = np.arange(5)[:, None], 2.0 * np.arange(5)

def predict(X):
    return np.full(shape=X.shape[0], fill_value=2)

class WrapPredict():
    def fit(self, X, y=None):
        self.is_fitted_ = True
        return self

    def predict(self, X):
        return predict(X)

predict_instance = WrapPredict().fit(X)

partial_dependence(predict_instance, X=X, features=0)
```
errors with
```
AttributeError: The following error was raised: 'WrapPredict' object has no attribute '__sklearn_tags__'. It seems that there are no classes that implement `__sklearn_tags__` in the MRO and/or all classes in the MRO call `super().__sklearn_tags__()`. Make sure to inherit from `BaseEstimator` which implements `__sklearn_tags__` (or alternatively define `__sklearn_tags__` but we don't recommend this approach). Note that `BaseEstimator` needs to be on the right side of other Mixins in the inheritance order.
```

### Proposal
Avoid at least the necessity of `__sklearn_tags__`, i.e., the above `WrapPredict` should pass.

#### Other
This is a bit related to #33003."
comment,33220,,"> which basically requires that it inherits from BaseEstimator.

What prevents you from inheriting from `BaseEstimator` ?
inheriting from `BaseEstimator` is now mandatory to be scikit-learn compatible, in the sense of passing estimator checks. I guess it's like that since 1.6.0 iirc."
comment,33220,,What if you hve a pytorch model? All you want is to call the predict method in order to inspect it. But now all of a sudden you need to inherit from BaseEstimator.
comment,33220,,ping @glemaitre @adrinjalali 
comment,33220,,"Either you need to implement the `__sklearn_tags__` method, or you need to inherit it.

The question is: ""Why would you need to implement `__sklearn_tags__`?"" The answer is that if you use your estimator inside another scikit-learn meta-estimator, many things were broken before.

That said, I also understand that one might not want to have to inherit or implement `__sklearn_tags__` because one might not want to use meta-estimators, thus adding a layer of complexity. It's exactly the same issue with `get_params` and `set_params`, which would not need to be implemented if you don't need to use `SearchCV`. This is exactly what's reflected in the code we use for testing with `MinimalClassifier` and co.: https://github.com/scikit-learn/scikit-learn/blob/bf223d9c21d3f85de90592f69b9a8710fecbc939/sklearn/utils/_testing.py#L1157-L1210

I think we need these implementations if we want things to work properly with scikit-learn. But maybe there is a way to allow minimal implementation without these methods if you're not using the parts of scikit-learn that require them, and to allow those components to raise errors if they don't have the expected interface. It would be a middle ground where doing easy things would be at no cost, and doing harder things would be at the cost of agreeing/implementing with the interface.

In short, I personally open the door to be flexible here."
comment,33220,,"Background: I am inclined to release a partial dependence implementation that only needs a predict function in my package model-diagnostics (internally, it‚Äôs already there).

I consider a modeler with the first models ready for inspection. Maybe, she does use another library that does not inherit from our BaseEstimator. Nothing she can change that easily. What shall she do? She wants a pd plot, but scikit-learn tells her, it does not work that way.
"
comment,33220,,"@lorentzenchr To be clear, I'm convinced about your arguments and I was uneasy to make it compulsory to inherit from `BaseEstimator`.

My only question is how do we change the code such that we raise whenever we really need the complexity with a ""pattern"" that allow us to have maintainable code in the long run.

My first thought would be to delegate to the `MetaEstimatorMixin` and `SearchCV` to do the check but I don't know if it is enough and there are other weird occurrences where we would shoot ourself in the foot due to the missing interface."
comment,33220,,"> [@lorentzenchr](https://github.com/lorentzenchr) To be clear, I'm convinced about your arguments and I was uneasy to make it compulsory to inherit from `BaseEstimator`.

Good and happy to know that!

> My only question is how do we change the code such that we raise whenever we really need the complexity with a ""pattern"" that allow us to have maintainable code in the long run.

In this instance, we need to change the logic of `check_is_fitted` by not only checking the negative (not fitted), but checking for the positive at the beginning.

Long term maintainability:
- Decide which features should work without BaseEstimator (difficult part)
- Implement tests checking our promise (easy part)"
comment,33220,,"I'm -1 on not requiring BaseEstimator in the long run. We even raise a warning in our check_estimator for it.

If the user isn't using scikit-learn at all, they don't need to inherit, if they use any of our tools, then they have the package installed and their class can inherit easily from BaseEstimator. So I don't see how this is a major issue."
comment,33220,,"Let's say, I am the user. I do
```py
from super_package import BestEstimatorInTheWorld

m = BestEstimatorInTheWorld().fit(X, y)

# Hm? Is it really that good? Let's inspect a bit.
# Fortunately, I trust the partial dependence implementation of scikit-learn.
from sklearn.inspection import partial_dependence

partial_dependence(m, X, features=0)  # üí• 
```
As a user, I do not have the power to easily let a package's estimator inherit from scikit-learn. My conclusion could be that I find scikit-learn annoying and not usable."
comment,33220,,"We gave a deprecation cycle for this already, with the right warnings. People have had at least a year to fix their code. If something's not working, it's not maintained at this point, and users might either consider using them, or downgrade scikit-learn."
comment,33220,,"That is one way of looking at it. With my own package, I am going in the opposite direction and only require a callable with signature `predict(X) -> array of length X.shape[0]`. Why care if an estimator is fitted or not, all we need is a callable.

I am not saying one way is better than the other, just that there a different way of looking at it."
comment,33220,,"It's actually quite hard to create estimators that are fully compliant with our API, much more than inheriting from BaseEstimator. This has been a challenge for multiple people, as noted by reactions in https://github.com/scikit-learn/scikit-learn/issues/33003

In addition, I feel like @lorentzenchr : there is value in being able to easily plug code into scikit-learn. The simple API of scikit-learn has actual been a structuring factor in the ecosystem (though it's been implemented in imperfect ways üòÄ  )

I'm in favor of having relaxed API that we try to support as much as we can (here partial_dependences gives a good example of place where it would be useful). This relaxed API wouldn't bring all the value of scikit-learn, but it would still be useful to users, IMHO."
issue,33219,HDBSCAN fails when using cluster_selection_epsilon,"### Describe the bug and give evidence about its user-facing impact

When using `HDBSCAN` with `cluster_selection_epsilon` set to at least 0.015 (for my dataset), I get a `TypeError`.


I only get the error when using `cluster_selection_epsilon`. I don't get this error when using the `hdbscan` package (version 0.8.41) directly.

### Steps/Code to Reproduce

Edit: I finally was able to reproduce the issue with synthetic data by playing around with `cluster_selection_epsilon` and you don't need precomputed distances:

```python
from sklearn.cluster import HDBSCAN
import numpy as np

rng = np.random.default_rng(0)

X = np.vstack([
    rng.normal(0, 0.2, size=(20, 2)),
    rng.normal(1, 0.2, size=(21, 2)),
])

hdb = HDBSCAN(
        min_cluster_size=5,
        min_samples=1,
        n_jobs=10,
        copy=True,
        allow_single_cluster=True,
        cluster_selection_epsilon=1
)

print(hdb.fit_predict(X))
``` 

The issue is only triggered with my specific data - I couldn't reproduce it with synthetic data. Notably, the same data works just fine with the `hdbscan` package.

```python
from hdbscan import HDBSCAN
import pandas as pd
# Bug only triggered with my specific dataset
df = pd.read_csv('bug.csv', header=None)

hdb = HDBSCAN(
        min_cluster_size=5,
        min_samples=1,
        n_jobs=10,
        metric='precomputed',
        copy=True,
        allow_single_cluster=True,
        cluster_selection_epsilon=0.015
)

print(hdb.fit_predict(df.values))
```

`bug.csv`:

```csv
0.0,0.0144234,0.014608,0.018432,0.2028331,1e-06,0.0146122,0.0108745,0.0238127,0.0289057,0.0292668
0.0144234,0.0,0.030085,0.0341493,0.2174494,0.0185183,0.0300855,0.0261329,0.0444496,0.0499432,0.0506157
0.014608,0.030085,0.0,0.0224902,0.2284748,0.0188262,0.022492,0.0147419,0.029195,0.0344591,0.0349043
0.018432,0.0341493,0.0224902,0.0,0.2288503,0.0238127,0.0035258,0.0147674,1e-06,0.0045079,0.0045573
0.2028331,0.2174494,0.2284748,0.2288503,0.0,0.2061707,0.220841,0.2293228,0.2397258,0.2299638,0.2552726
1e-06,0.0185183,0.0188262,0.0238127,0.2061707,0.0,0.0188313,0.013986,0.0238127,0.0289057,0.0292668
0.0146122,0.0300855,0.022492,0.0035258,0.220841,0.0188313,0.0,0.0186034,0.0044997,0.0091034,0.0092055
0.0108745,0.0261329,0.0147419,0.0147674,0.2293228,0.013986,0.0186034,0.0,0.0190924,0.0241502,0.0244496
0.0238127,0.0444496,0.029195,1e-06,0.2397258,0.0238127,0.0044997,0.0190924,0.0,0.0045079,0.0045573
0.0289057,0.0499432,0.0344591,0.0045079,0.2299638,0.0289057,0.0091034,0.0241502,0.0045079,0.0,0.0092227
0.0292668,0.0506157,0.0349043,0.0045573,0.2552726,0.0292668,0.0092055,0.0244496,0.0045573,0.0092227,0.0
```

### Expected Results

Using the `hdbscan` package:

```
[ 0  0  0  0 -1  0  0  0  0  0  0]
```

### Actual Results

```
Traceback (most recent call last):
  File ""bug.py"", line 16, in <module>
    print(hdb.fit_predict(df.values))
          ~~~~~~~~~~~~~~~^^^^^^^^^^^
  File "".conda/lib/python3.14/site-packages/sklearn/cluster/_hdbscan/hdbscan.py"", line 920, in fit_predict
    self.fit(X)
    ~~~~~~~~^^^
  File "".conda/lib/python3.14/site-packages/sklearn/base.py"", line 1336, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "".conda/lib/python3.14/site-packages/sklearn/cluster/_hdbscan/hdbscan.py"", line 866, in fit
    self.labels_, self.probabilities_ = tree_to_labels(
                                        ~~~~~~~~~~~~~~^
        self._single_linkage_tree_,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
        self.max_cluster_size,
        ^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File ""sklearn/cluster/_hdbscan/_tree.pyx"", line 61, in sklearn.cluster._hdbscan._tree.tree_to_labels
  File ""sklearn/cluster/_hdbscan/_tree.pyx"", line 75, in sklearn.cluster._hdbscan._tree.tree_to_labels
  File ""sklearn/cluster/_hdbscan/_tree.pyx"", line 751, in sklearn.cluster._hdbscan._tree._get_clusters
  File ""sklearn/cluster/_hdbscan/_tree.pyx"", line 626, in sklearn.cluster._hdbscan._tree.epsilon_search
  File ""sklearn/cluster/_hdbscan/_tree.pyx"", line 588, in sklearn.cluster._hdbscan._tree.traverse_upwards
TypeError: only 0-dimensional arrays can be converted to Python scalars
```

### Versions

```shell
System:
    python: 3.14.2 | packaged by conda-forge | (main, Jan 26 2026, 19:56:00) [GCC 14.3.0]
executable: .conda/bin/python
   machine: Linux-5.15.0-119-generic-x86_64-with-glibc2.35

Python dependencies:
      sklearn: 1.8.0
          pip: 25.3
   setuptools: 80.10.2
        numpy: 2.4.2
        scipy: 1.17.0
       Cython: None
       pandas: 3.0.0
   matplotlib: 3.10.8
       joblib: 1.5.3
threadpoolctl: 3.6.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 64
         prefix: libscipy_openblas
       filepath: .conda/lib/python3.14/site-packages/numpy.libs/libscipy_openblas64_-096271d3.so
        version: 0.3.31.dev
threading_layer: pthreads
   architecture: SkylakeX

       user_api: blas
   internal_api: openblas
    num_threads: 64
         prefix: libscipy_openblas
       filepath: .conda/lib/python3.14/site-packages/scipy.libs/libscipy_openblas-6cdc3b4a.so
        version: 0.3.30
threading_layer: pthreads
   architecture: SkylakeX

       user_api: openmp
   internal_api: openmp
    num_threads: 96
         prefix: libgomp
       filepath: .conda/lib/python3.14/site-packages/scikit_learn.libs/libgomp-e985bcbb.so.1.0.0
        version: None
```

### Interest in fixing the bug

I don't want to create a PR because I don't know the codebase at all.

I asked ChatGPT for possible causes and it thinks the bug might be related to tied distances, resulting in competing branches and epsilon intersects a plateau."
comment,33219,,"## Root cause identified

I was able to reproduce the crash and traced it to tied distances (plateaus) in the condensed tree, e.g. repeated 1e-06 values in the provided data. In these cases, expressions such as:

cluster_tree[cluster_tree['child'] == leaf]['parent']

1 / cluster_tree[cluster_tree['child'] == parent]['value']

can return arrays with length greater than 1 rather than scalars. The current implementation assumes a scalar and later attempts to cast the result to a Python float, which raises:

TypeError: only 0-dimensional arrays can be converted to Python scalars

## Proposed approach

In local testing, adding defensive handling for these cases avoids the crash by:

explicitly handling multi-match results by selecting a deterministic representative, guarding against empty selections, and applying the same logic in both traverse_upwards and the parent_eps computation. With this change, the error no longer occurs on the reported dataset or related tests.
Note that when ties are present, the resulting labels may differ from the reference hdbscan package; the intent here is robustness (avoiding crashes) rather than strict behavioral equivalence under tied distances.

I also noticed a related latent issue at:
eps = 1 / distances[leaf_nodes][0]


which can similarly fail if leaf_nodes is empty; I handled this defensively in local testing as well."
comment,33219,,"Thanks @Thomas1664 for reporting the bug, I am also able to reproduce it. 

It seems that the issue is that `cluster_tree[cluster_tree['child'] == leaf]['parent']` returns a list while `parent` is defined as an int. Simply using `parent = cluster_tree[cluster_tree['child'] == leaf]['parent'][0]` instead should do the trick. 

I believe there is a similar problem [here](https://github.com/scikit-learn/scikit-learn/blob/33187195c2cc862ad775244af9cec28ccdf32bad/sklearn/cluster/_hdbscan/_tree.pyx#L595) with `parent_eps`, for which `parent_eps = 1 / cluster_tree[cluster_tree['child'] == parent]['value'][0]` should be used. 

Also I noticed while testing that the [`traverse_upwards`](https://github.com/scikit-learn/scikit-learn/blob/33187195c2cc862ad775244af9cec28ccdf32bad/sklearn/cluster/_hdbscan/_tree.pyx#L578) function is not covered by the tests."
pull_request,33218,TST Move common binary Display class tests to `test_common_curve_display.py`,"#### Reference Issues/PRs
Related to #33217
Moving the test changes from #30508 as requested


#### What does this implement/fix? Explain your changes.

* In #30508 I noticed tests that were common between binary Display classes `PrecisionRecallDisplay` and `RocCurveDisplay` thus I moved them to common test file to avoid duplication and we can easily add new displays to the parametrization, as we add `from_cv_results` method to more display classes
* Most tests relate to the new `from_cv_results` method 

Summary of migrated tests

`test_validate_plot_params` -> `test_display_validate_plot_params`

`test_roc_curve_plot_legend_label` -> `test_display_plot_legend_label`

`test_roc_curve_from_cv_results_legend_label` -> `test_display_from_cv_results_legend_label`

`test_roc_curve_from_cv_results_param_validation` -> `test_display_from_cv_results_param_validation`

`test_roc_curve_from_cv_results_pos_label_inferred` -> `test_display_from_cv_results_pos_label_inferred`

`test_roc_curve_display_from_cv_results_curve_kwargs` -> `test_display_from_cv_results_curve_kwargs`

New test: `test_display_default_name`

#### AI usage disclosure
<!--
If AI tools were involved in creating this PR, please check all boxes that apply
below and make sure that you adhere to our Automated Contributions Policy:
https://scikit-learn.org/dev/developers/contributing.html#automated-contributions-policy
-->
I used AI assistance for:
- [ ] Code generation (e.g., when writing an implementation or fixing a bug)
- [ ] Test/benchmark generation
- [ ] Documentation (including examples)
- [ ] Research and understanding


#### Any other comments?

cc @jeremiedbb @StefanieSenger "
comment,33218,,"Codecov is failing for `test_common_curve_display.py` due to precision recall specific lines that obviously are not run here. I would rather keep them to avoid nightmare merge conflicts with https://github.com/scikit-learn/scikit-learn/pull/33217

Note it was nice to have the tests in #33217, at least at the start, so they can all be run and I could make sure the new code worked."
pull_request,33217,TST Add `CalibrationDisplay` to `test_common_curve_display.py`,"#### Reference Issues/PRs
Noticed when separating test changes out from #30508


#### What does this implement/fix? Explain your changes.

* Adds `CalibrationDisplay` to relevant common tests
* Improves test names and docstrings such that they fit on one line
* Removes redundant test (possibly there via bad merge)

#### AI usage disclosure
<!--
If AI tools were involved in creating this PR, please check all boxes that apply
below and make sure that you adhere to our Automated Contributions Policy:
https://scikit-learn.org/dev/developers/contributing.html#automated-contributions-policy
-->
I used AI assistance for:
- [ ] Code generation (e.g., when writing an implementation or fixing a bug)
- [ ] Test/benchmark generation
- [ ] Documentation (including examples)
- [ ] Research and understanding


#### Any other comments?
 cc @AnneBeyer @StefanieSenger in case you have time to review :pray: 

<!--
Thank you for your patience. Changes to scikit-learn require careful
attention, but with limited maintainer time, not every contribution can be reviewed
quickly.
For more information and tips on improving your pull request, see:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
"
issue,33216,"‚ö†Ô∏è CI failed on Unit tests Linux x86-64 pylatest_free_threaded (last failure: Feb 08, 2026) ‚ö†Ô∏è","**CI is still failing on [Unit tests Linux x86-64 pylatest_free_threaded](https://github.com/scikit-learn/scikit-learn/actions/runs/21791666506/job/62872061940)** (Feb 08, 2026)
- test_estimators[GradientBoostingRegressor(n_estimators=5)-check_regressors_train]
- test_estimators[GradientBoostingRegressor(n_estimators=5)-check_regressors_train(readonly_memmap=True)]
- test_estimators[GradientBoostingRegressor(n_estimators=5)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[GridSearchCV(cv=2,error_score='raise',estimator=Ridge(),param_grid={'alpha':[0.1,1.0]})-check_regressors_train]
- test_estimators[GridSearchCV(cv=2,error_score='raise',estimator=Ridge(),param_grid={'alpha':[0.1,1.0]})-check_regressors_train(readonly_memmap=True)]
- test_estimators[GridSearchCV(cv=2,error_score='raise',estimator=Ridge(),param_grid={'alpha':[0.1,1.0]})-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[GridSearchCV(cv=2,error_score='raise',estimator=Pipeline(steps=[('pca',PCA()),('ridge',Ridge())]),param_grid={'ridge__alpha':[0.1,1.0]})-check_regressors_train]
- test_estimators[GridSearchCV(cv=2,error_score='raise',estimator=Pipeline(steps=[('pca',PCA()),('ridge',Ridge())]),param_grid={'ridge__alpha':[0.1,1.0]})-check_regressors_train(readonly_memmap=True)]
- test_estimators[GridSearchCV(cv=2,error_score='raise',estimator=Pipeline(steps=[('pca',PCA()),('ridge',Ridge())]),param_grid={'ridge__alpha':[0.1,1.0]})-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[HalvingGridSearchCV(cv=2,error_score='raise',estimator=Ridge(),min_resources='smallest',param_grid={'alpha':[0.1,1.0]},random_state=0)-check_regressors_train]
- test_estimators[HalvingGridSearchCV(cv=2,error_score='raise',estimator=Ridge(),min_resources='smallest',param_grid={'alpha':[0.1,1.0]},random_state=0)-check_regressors_train(readonly_memmap=True)]
- test_estimators[HalvingGridSearchCV(cv=2,error_score='raise',estimator=Ridge(),min_resources='smallest',param_grid={'alpha':[0.1,1.0]},random_state=0)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[HalvingGridSearchCV(cv=2,error_score='raise',estimator=Pipeline(steps=[('pca',PCA()),('ridge',Ridge())]),min_resources='smallest',param_grid={'ridge__alpha':[0.1,1.0]},random_state=0)-check_regressors_train]
- test_estimators[HalvingGridSearchCV(cv=2,error_score='raise',estimator=Pipeline(steps=[('pca',PCA()),('ridge',Ridge())]),min_resources='smallest',param_grid={'ridge__alpha':[0.1,1.0]},random_state=0)-check_regressors_train(readonly_memmap=True)]
- test_estimators[HalvingGridSearchCV(cv=2,error_score='raise',estimator=Pipeline(steps=[('pca',PCA()),('ridge',Ridge())]),min_resources='smallest',param_grid={'ridge__alpha':[0.1,1.0]},random_state=0)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[HalvingRandomSearchCV(cv=2,error_score='raise',estimator=Ridge(),param_distributions={'alpha':[0.1,1.0]},random_state=0)-check_regressors_train]
- test_estimators[HalvingRandomSearchCV(cv=2,error_score='raise',estimator=Ridge(),param_distributions={'alpha':[0.1,1.0]},random_state=0)-check_regressors_train(readonly_memmap=True)]
- test_estimators[HalvingRandomSearchCV(cv=2,error_score='raise',estimator=Ridge(),param_distributions={'alpha':[0.1,1.0]},random_state=0)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[HalvingRandomSearchCV(cv=2,error_score='raise',estimator=Pipeline(steps=[('pca',PCA()),('ridge',Ridge())]),param_distributions={'ridge__alpha':[0.1,1.0]},random_state=0)-check_regressors_train]
- test_estimators[HalvingRandomSearchCV(cv=2,error_score='raise',estimator=Pipeline(steps=[('pca',PCA()),('ridge',Ridge())]),param_distributions={'ridge__alpha':[0.1,1.0]},random_state=0)-check_regressors_train(readonly_memmap=True)]
- test_estimators[HalvingRandomSearchCV(cv=2,error_score='raise',estimator=Pipeline(steps=[('pca',PCA()),('ridge',Ridge())]),param_distributions={'ridge__alpha':[0.1,1.0]},random_state=0)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[HistGradientBoostingRegressor(max_iter=5,min_samples_leaf=5)-check_regressors_train]
- test_estimators[HistGradientBoostingRegressor(max_iter=5,min_samples_leaf=5)-check_regressors_train(readonly_memmap=True)]
- test_estimators[HistGradientBoostingRegressor(max_iter=5,min_samples_leaf=5)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[HuberRegressor(max_iter=5)-check_regressors_train]
- test_estimators[HuberRegressor(max_iter=5)-check_regressors_train(readonly_memmap=True)]
- test_estimators[HuberRegressor(max_iter=5)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[KNeighborsRegressor(metric='precomputed')-check_regressors_train]
- test_estimators[KNeighborsRegressor(metric='precomputed')-check_regressors_train(readonly_memmap=True)]
- test_estimators[KNeighborsRegressor(metric='precomputed')-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[KernelRidge()-check_regressors_train]
- test_estimators[KernelRidge()-check_regressors_train(readonly_memmap=True)]
- test_estimators[KernelRidge()-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[Lars()-check_regressors_train]
- test_estimators[Lars()-check_regressors_train(readonly_memmap=True)]
- test_estimators[Lars()-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[LarsCV(cv=3,max_iter=5)-check_regressors_train]
- test_estimators[LarsCV(cv=3,max_iter=5)-check_regressors_train(readonly_memmap=True)]
- test_estimators[LarsCV(cv=3,max_iter=5)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[Lasso(max_iter=5)-check_regressors_train]
- test_estimators[Lasso(max_iter=5)-check_regressors_train(readonly_memmap=True)]
- test_estimators[Lasso(max_iter=5)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[LassoCV(cv=3,max_iter=5)-check_regressors_train]
- test_estimators[LassoCV(cv=3,max_iter=5)-check_regressors_train(readonly_memmap=True)]
- test_estimators[LassoCV(cv=3,max_iter=5)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[LassoLars(max_iter=5)-check_regressors_train]
- test_estimators[LassoLars(max_iter=5)-check_regressors_train(readonly_memmap=True)]
- test_estimators[LassoLars(max_iter=5)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[LassoLarsCV(cv=3,max_iter=5)-check_regressors_train]
- test_estimators[LassoLarsCV(cv=3,max_iter=5)-check_regressors_train(readonly_memmap=True)]
- test_estimators[LassoLarsCV(cv=3,max_iter=5)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[LassoLarsIC(max_iter=5,noise_variance=1.0)-check_regressors_train]
- test_estimators[LassoLarsIC(max_iter=5,noise_variance=1.0)-check_regressors_train(readonly_memmap=True)]
- test_estimators[LassoLarsIC(max_iter=5,noise_variance=1.0)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[LinearRegression()-check_regressors_train]
- test_estimators[LinearRegression()-check_regressors_train(readonly_memmap=True)]
- test_estimators[LinearRegression()-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[LinearSVR(max_iter=20)-check_regressors_train]
- test_estimators[LinearSVR(max_iter=20)-check_regressors_train(readonly_memmap=True)]
- test_estimators[LinearSVR(max_iter=20)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[MultiOutputRegressor(estimator=Ridge())-check_regressors_train]
- test_estimators[MultiOutputRegressor(estimator=Ridge())-check_regressors_train(readonly_memmap=True)]
- test_estimators[MultiOutputRegressor(estimator=Ridge())-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[MultiTaskElasticNet(max_iter=5)-check_regressors_train]
- test_estimators[MultiTaskElasticNet(max_iter=5)-check_regressors_train(readonly_memmap=True)]
- test_estimators[MultiTaskElasticNet(max_iter=5)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[MultiTaskElasticNetCV(cv=3,max_iter=5)-check_regressors_train]
- test_estimators[MultiTaskElasticNetCV(cv=3,max_iter=5)-check_regressors_train(readonly_memmap=True)]
- test_estimators[MultiTaskElasticNetCV(cv=3,max_iter=5)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[MultiTaskLasso(max_iter=5)-check_regressors_train]
- test_estimators[MultiTaskLasso(max_iter=5)-check_regressors_train(readonly_memmap=True)]
- test_estimators[MultiTaskLasso(max_iter=5)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[MultiTaskLassoCV(cv=3,max_iter=5)-check_regressors_train]
- test_estimators[MultiTaskLassoCV(cv=3,max_iter=5)-check_regressors_train(readonly_memmap=True)]
- test_estimators[MultiTaskLassoCV(cv=3,max_iter=5)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[NuSVR()-check_regressors_train]
- test_estimators[NuSVR()-check_regressors_train(readonly_memmap=True)]
- test_estimators[NuSVR()-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[OrthogonalMatchingPursuit()-check_regressors_train]
- test_estimators[OrthogonalMatchingPursuit()-check_regressors_train(readonly_memmap=True)]
- test_estimators[OrthogonalMatchingPursuit()-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[OrthogonalMatchingPursuitCV(cv=3)-check_regressors_train]
- test_estimators[OrthogonalMatchingPursuitCV(cv=3)-check_regressors_train(readonly_memmap=True)]
- test_estimators[OrthogonalMatchingPursuitCV(cv=3)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[PassiveAggressiveRegressor(max_iter=5)-check_regressors_train]
- test_estimators[PassiveAggressiveRegressor(max_iter=5)-check_regressors_train(readonly_memmap=True)]
- test_estimators[PassiveAggressiveRegressor(max_iter=5)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[Pipeline(steps=[('scaler',StandardScaler()),('final_estimator',Ridge())])-check_regressors_train]
- test_estimators[Pipeline(steps=[('scaler',StandardScaler()),('final_estimator',Ridge())])-check_regressors_train(readonly_memmap=True)]
- test_estimators[Pipeline(steps=[('scaler',StandardScaler()),('final_estimator',Ridge())])-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[PoissonRegressor(max_iter=5)-check_regressors_train]
- test_estimators[PoissonRegressor(max_iter=5)-check_regressors_train(readonly_memmap=True)]
- test_estimators[PoissonRegressor(max_iter=5)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[QuantileRegressor()-check_regressors_train]
- test_estimators[QuantileRegressor()-check_regressors_train(readonly_memmap=True)]
- test_estimators[QuantileRegressor()-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[RANSACRegressor(estimator=LinearRegression(),max_trials=10)-check_regressors_train]
- test_estimators[RANSACRegressor(estimator=LinearRegression(),max_trials=10)-check_regressors_train(readonly_memmap=True)]
- test_estimators[RANSACRegressor(estimator=LinearRegression(),max_trials=10)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[RandomizedSearchCV(cv=2,error_score='raise',estimator=Ridge(),param_distributions={'alpha':[0.1,1.0]},random_state=0)-check_regressors_train]
- test_estimators[RandomizedSearchCV(cv=2,error_score='raise',estimator=Ridge(),param_distributions={'alpha':[0.1,1.0]},random_state=0)-check_regressors_train(readonly_memmap=True)]
- test_estimators[RandomizedSearchCV(cv=2,error_score='raise',estimator=Ridge(),param_distributions={'alpha':[0.1,1.0]},random_state=0)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[RandomizedSearchCV(cv=2,error_score='raise',estimator=Pipeline(steps=[('pca',PCA()),('ridge',Ridge())]),param_distributions={'ridge__alpha':[0.1,1.0]},random_state=0)-check_regressors_train]
- test_estimators[RandomizedSearchCV(cv=2,error_score='raise',estimator=Pipeline(steps=[('pca',PCA()),('ridge',Ridge())]),param_distributions={'ridge__alpha':[0.1,1.0]},random_state=0)-check_regressors_train(readonly_memmap=True)]
- test_estimators[RandomizedSearchCV(cv=2,error_score='raise',estimator=Pipeline(steps=[('pca',PCA()),('ridge',Ridge())]),param_distributions={'ridge__alpha':[0.1,1.0]},random_state=0)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[RegressorChain(cv=3,estimator=Ridge())-check_regressors_train]
- test_estimators[RegressorChain(cv=3,estimator=Ridge())-check_regressors_train(readonly_memmap=True)]
- test_estimators[RegressorChain(cv=3,estimator=Ridge())-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[Ridge()-check_regressors_train]
- test_estimators[Ridge()-check_regressors_train(readonly_memmap=True)]
- test_estimators[Ridge()-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[RidgeCV()-check_regressors_train]
- test_estimators[RidgeCV()-check_regressors_train(readonly_memmap=True)]
- test_estimators[RidgeCV()-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[SGDRegressor(max_iter=5)-check_regressors_train]
- test_estimators[SGDRegressor(max_iter=5)-check_regressors_train(readonly_memmap=True)]
- test_estimators[SGDRegressor(max_iter=5)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[SVR(kernel='precomputed')-check_regressors_train]
- test_estimators[SVR(kernel='precomputed')-check_regressors_train(readonly_memmap=True)]
- test_estimators[SVR(kernel='precomputed')-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[StackingRegressor(cv=3,estimators=[('est1',DecisionTreeRegressor(max_depth=3,random_state=0)),('est2',DecisionTreeRegressor(max_depth=3,random_state=1))])-check_regressors_train]
- test_estimators[StackingRegressor(cv=3,estimators=[('est1',DecisionTreeRegressor(max_depth=3,random_state=0)),('est2',DecisionTreeRegressor(max_depth=3,random_state=1))])-check_regressors_train(readonly_memmap=True)]
- test_estimators[StackingRegressor(cv=3,estimators=[('est1',DecisionTreeRegressor(max_depth=3,random_state=0)),('est2',DecisionTreeRegressor(max_depth=3,random_state=1))])-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[TheilSenRegressor(max_iter=5,max_subpopulation=100)-check_regressors_train]
- test_estimators[TheilSenRegressor(max_iter=5,max_subpopulation=100)-check_regressors_train(readonly_memmap=True)]
- test_estimators[TheilSenRegressor(max_iter=5,max_subpopulation=100)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[TweedieRegressor(max_iter=5)-check_regressors_train]
- test_estimators[TweedieRegressor(max_iter=5)-check_regressors_train(readonly_memmap=True)]
- test_estimators[TweedieRegressor(max_iter=5)-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_estimators[VotingRegressor(estimators=[('est1',DecisionTreeRegressor(max_depth=3,random_state=0)),('est2',DecisionTreeRegressor(max_depth=3,random_state=1))])-check_regressors_train]
- test_estimators[VotingRegressor(estimators=[('est1',DecisionTreeRegressor(max_depth=3,random_state=0)),('est2',DecisionTreeRegressor(max_depth=3,random_state=1))])-check_regressors_train(readonly_memmap=True)]
- test_estimators[VotingRegressor(estimators=[('est1',DecisionTreeRegressor(max_depth=3,random_state=0)),('est2',DecisionTreeRegressor(max_depth=3,random_state=1))])-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
- test_check_estimator
- test_check_estimator_clones
- test_check_estimator_pairwise"
comment,33216,,"First nice to see that the automatic issue with the name fix works, it was done in https://github.com/scikit-learn/scikit-learn/pull/33203 cc @FrancoisPgm üéâ 

The failure looks like a variant of https://github.com/scikit-learn/scikit-learn/issues/32393#issuecomment-3858235444. There could still be some free-threaded specific aspects on top though, in particular one more test fails."
comment,33216,,"## CI is no longer failing! ‚úÖ

[Successful run](https://github.com/scikit-learn/scikit-learn/actions/runs/21891751739/job/63198953642) on Feb 11, 2026"
comment,33216,,Fixed by #33221.
pull_request,33215,ENH: add ExtremeLearningClassifier/Regressor estimators,"## Introduction

As part of a new 3-year DOE SC ASCR applied math grant, we've been performing research on gradient free neural network architectures that strike a balance between affordable/efficient (i.e., non-GPU) hardware/compute cost and model accuracy. We found that the open source scientific Python community was lacking some very fundamental estimators in this regard. In particular, there are currently no well-maintained multi-layer Extreme Learning Machine (ELM) network  or random vector functional link (RVFL) libraries that we are aware of. https://github.com/thieu1995/GrafoRVFL currently use a GPL license, and only supports single-layer RVFL networks. Here, we propose to add support for two types of fundamental gradient-free estimators: `ExtremeLearningClassifier` and `ExtremeLearningRegressor`, which also offer parameters to switch over to RVFL. A wider range of estimators is being developed at https://github.com/lanl/GFDL, but the more specialized variants may not be of sufficiently broad interest for this project at this time. Our team consists of myself (Emma Viani), Navamita Ray, and Tyler Reddy (CS team at LANL); and Kyle Luh (CU Boulder) and Konstantinos Spiliopoulos (Boston University) on the mathematics side.

## Relevance and ""fit"" of the new estimators

A key question is whether these two estimators are of sufficiently broad interest for inclusion in `scikit-learn`. Their gradient-free nature means that they are generally quite efficient and don't require specialized hardware, consistent with a large number of heavily-used estimators like `RandomForestClassifier`. In terms of the breadth of their usage in the community, the table below summarizes a small sampling of relevant papers and citation counts (there are many others...).


Topic| DOI | Citations (Google Scholar, Jan 18, 2026)
-- | -- | --
Review of recent ELM research | https://doi.org/10.1007/S11042-021-11007-7 | 630
Evaluating RVFLs on common datasets | https://doi.org/10.1016/j.ins.2015.09.025 | 498
Application of RVFLs to electricity load forecasting | https://doi.org/10.1016/j.ins.2015.11.039 | 336
Exploration of deep RVFL architectures | https://doi.org/10.1016/j.patcog.2021.107978 | 277
Introduction of ELMs (the basic gradient-free NN concept) | https://doi.org/10.1016/j.neucom.2005.12.126 | 16,475

## How do these new estimators work and avoid gradients/backpropagation?

The tradeoff for avoiding backpropagation/GPUs is largely related to randomness--the weights of all neurons are randomly set and frozen during training, with the exception of the connection between the final layer and the output. This simplification allows a closed form solution that can proceed either via Moore-Penrose pseudoinverse (i.e., `np.pinv`) or via the various algorithms underlying ridge regression (literally `Ridge` in `scikit-learn`). A simple diagram of an RVFL demonstrates one additional twist--a direct link between the inputs and the outputs (source: https://doi.org/10.1016/j.patcog.2021.107978) that the community suggests is a form of regularization:

<img width=""386"" height=""241"" alt=""Image"" src=""https://github.com/user-attachments/assets/0c272012-95c7-4b4a-9665-9f2feccd2045"" />

As a result of the randomness in the training of these networks, we have to provide a wide range of options for setting the initial weights of the hidden layers, since they are frozen once set and can substantially affect estimator performance.

## Model structure and associated pitfalls

### Multi-layer behavior

Our ‚Äúdeep‚Äù variants of the traditional ELM are characterized by a stacked hierarchy of hidden layers. 

<img width=""357"" height=""343"" alt=""image"" src=""https://github.com/user-attachments/assets/362cc341-b6b4-40ac-b6b5-6693c7c40b86"" />

Each subsequent layer‚Äôs activations are generated from the previous layer (and, for the ‚Äúdeep RVFL‚Äù variant, concatenated with the original inputs). This produces a progressively larger (and potentially more correlated) feature map, which is part of what makes numerical conditioning a concern.

### Partial fit

Our `partial_fit()` implementation is mathematically equivalent to accumulating the normal equation terms for linear least squares.

Least squares solution:

$$
D^+ y = (D^T D)^{-1} D^T y
$$

Instead of storing the full design matrix, we incrementally accumulate the normal-equation terms $D^TD$ and $D^Ty$ across mini-batches.

Update rule:
Consider the summation representation of our gram matrix ($D^TD$):

$$
(D^T D)_{ij} = \sum_k D^T_{ik} D_{kj}
$$

Now suppose the full design matrix is formed by vertically stacking two mini-batches, written as 

$$
D = \begin{bmatrix}
B_1 \\
B_2
\end{bmatrix}
$$

$$
\begin{bmatrix}
B_1 B_2
\end{bmatrix}  \begin{bmatrix}
B_1 \\
B_2
\end{bmatrix} = B_1^T B_1 + B_2^T B_2
$$

It's evident that our two normal-equations _can_ be updated by simply summing the successive batches.

We do not store the full design matrix `D` across batches; we only store the gram and moment matrices `(A,B)`.This keeps memory linear in the number of batches, but it means the implementation is ultimately limited by the size of `A`, which scales with the number of features in the design matrix. `partial_fit()` is often significantly slower than `fit()` because we must repeatedly solve a growing linear system as batches arrive.

### Ill-conditioning and rank deficiency

Because D is a concatenation of random features (and optionally the original inputs), it can potentially be highly collinear or rank deficient, depending on activation/weight initialization, layer widths, and whether direct links are enabled. When $D^TD$ is ill-conditioned, the unregularized pseudoinverse can be numerically unstable. `partial_fit()` can diverge from the full `fit()` result (despite being mathematically equivalent in exact arithmetic) because of this.

In practice, ridge regularization and/or a larger pseudoinverse cutoff (rtol) often improves stability, but does not eliminate all pathological cases.

We've included an xfail test case documenting a parameter combination that reliably produces a rank-deficient design matrix and therefore a reproducible divergence between fit and partial_fit.

## Added weight initializers and activations

Because of the inherent design of the ELM, the distribution of fixed weights is a necessarily tunable hyperparameter that can substantially affect performance. To this end, we add multiple weight initialization options to the existing neural network base class and expand the available in-place activation functions.

## API and design considerations

One nice thing about the design is that it provides direct access to two of the most popular gradient-free neural networks (ELM and RVFL), with interchange between them available via a single parameter (when `direct_links=True` you have an RVFL, otherwise an ELM). Some debate could be had about whether it makes sense to only offer the classical forms (single-layer only) of ELM and RVFL, but we ultimately found considerable literature exploring the deep/multi-layer versions of each type of architecture, and wanted access to well-maintained code that could be used to reproduce such studies.

Some debate may be needed on the pass-through to `Ridge` arguments we use internally. `solver='auto'` is currently used, and may be a sensible default. However, as we have experienced in comparing with numerical experiments with mathematicians, it is likely of interest to researchers to have the ability to control the solver used by `Ridge` to be able to closely compare these gradient-free models with i.e., backpropagated models that use specific solvers.

## Performance of the estimators on common datasets

In the table below, we report some estimator performances for these new models on common classification and regression datasets. Note that these are not refined results (in the absence of substantial hyperopt).

Dataset |  Estimator  | Metric = Value  
-- | -- | --
Boston Housing | ExtremeLearningRegressor | $R^2$ = 0.8583
Breast Cancer Wisconsin | ExtremeLearningClassifier | ROC AUC = 0.9697


### Licensing Considerations

We did not use LLMs to draft or to assist in the drafting of this code, and did not consult copyleft/GPL-licensed code, apart from checking the executed output from the `graforvfl` library. Note that we've also received an informal positive response from the grafo team that they will switch to a more permissive license in the future to alleviate concerns about this: https://github.com/thieu1995/GrafoRVFL/issues/1#issuecomment-3253116472."
comment,33215,,"I think we already had a discussion about the inclusion of ELMs a long, long while ago, but this was on the scikit-learn mailing list and I could not find a related discussion our github issue tracker). At the time, we discussed an implementation strategy that would be similar to the ones implemented in `sklearn.kernel_approximation`. However, it's true that this approach does not scale to large `n_samples` values: it would require allocating a large `(n_samples, hidden_dim)` array which can be prohibitively expensive from a memory usage point of view.

Few questions before we discuss inclusion in more details:

- Could you update the broken link to the first reference in the table in the description?

- Do you have practical experience with ELMs/RVFL? For which kinds of problems have you observed a favorable (train speed, prediction latency) vs accuracy tradeoff vs alternatives (linear models on top of kernel approximations, MLPs, tree-based models)? It would be great to evaluate how they fair on tabular data benchmarks such as https://huggingface.co/spaces/TabArena/leaderboard

- Have you evaluated scikit-elm (https://github.com/akusok/scikit-elm) that seems to provide scikit-learn API compatible ELM implementations?
"
comment,33215,,"I'll let Emma respond in more detail next week, but for a few quick notes:

> Could you update the broken link to the first reference in the table in the description?

Hmm, all the links seem to work for me? (Emma may have fixed it after your comment)

> Do you have practical experience with ...

Interesting, I wasn't aware of that leaderboard/ranking system, maybe we should see if we can get ranked in there indeed if it isn't too painful to do since this is a multi-year thrust to prove these kinds of networks out.

We've been looking for good ways to demonstrate the utility of the gradient-free networks, though our approach was mostly to show that we could do quite well on i.e., image recognition problems without needing a GPU/backpropagation (i.e., Emma has some preliminary results for some image recognition datasets that look promising relative to the best known non-backprop models). So the angle was more like ""doing well on problems that normally require GPUs to do well, but on CPU.""

I believe Emma had observed on some image recognition datasets that training time was exceptionally good relative to common competitors (tree-based, MLPs), while the tradeoff seemed to come at inference time. But Emma can speak to the details better than I. She has a few plots I think (maybe we could share for a straightforward dataset or two using this feature branch as the source of the estimators).

> Have you evaluated scikit-elm 

I don't think we did, but we were fairly confident that there were no great open source options for multi-layer ELMs/RVFLs. From some grepping around in their code base, it seems like this is still the case--`ELMClassifier` over there only accepts `n_neurons` as an integer for init, and I don't see any evidence that they support multi-layer architectures. Same for direct links (skip layer)--they don't seem to offer the option to switch over to RVFLs, which is potentially quite important for regularization. I believe Emma tried to match the API of `MLPClassifier` in the area of multi-layer API (i.e., `hidden_layer_sizes`, etc.) quite closely.

In their docs, at `doc/methodology.rst`, I do see the note on multi-layer ELMs pasted below. So, it sounds like they agree with us in principle that these are useful, they just never got around to implementing it.

```
 60 (upcoming) Deep ELM
 61 -------------------
 62 
 63 There are multi-layer extensions of ELM that actually make sense and work well
```



"
issue,33214,RFC change default `scoring` of `LogisticRegressionCV` from  accuracy to logloss,"I propose to change the default value of `LogisticRegressionCV(scoring=None)` (`None` uses accuracy) to `LogisticRegressionCV(scoring=neg_log_loss)`.

Reason:
- Logistic Regression minimizes (penalized) log loss when fitting.
- Scoring with accuracy calls `LogisticRegression.predict` which applies an arbitrary probability threshold of 50%.
- Other estimators also use log loss by default for similar purposes (early stopping)
  - `HistGradientBoostingClassifier` defaults to `scoring=loss` which uses log loss.
  - `GradientBoostingClassifier` uses training loss function which defaults to log loss.

@scikit-learn/core-devs @scikit-learn/communication-team @scikit-learn/documentation-team"
comment,33214,,"I think implementing this correction makes definitely sense.

Now, as for SLEP 25, I don't know how the project could transition easily without breaking people's and system's habits and assumptions. To me, this looks important enough to be part of 2.0."
comment,33214,,"Thanks for the work you've done so far. The goal of this comment is to set expectations.

Deciding on new features or substantial changes is a lengthy process. It frequently happens that no maintainer is available to take on this task right now.

Please do not create a Pull Request before a decision has been made regarding the proposed work. Making this decision can often take a significant amount of time and effort.
"
pull_request,33213,DOC: clarify hard requirements for estimator compatibility,"This PR improves the ""Rolling your own estimator"" documentation by clarifying
that full scikit-learn compatibility is defined by passing `check_estimator`
and by making some hard requirements explicit.

The changes focus on emphasis and early discovery of incompatibilities,
without adding new documentation or guides.

Fixes #33003
Related to #32910"
comment,33213,,"Please read https://github.com/scikit-learn/scikit-learn/issues/33003#issuecomment-3816197068
"
pull_request,33212,MAINT use the rapidsai channel first in the CUDA CI config,Apparently this is needed to get deterministically non-failing mamba dependency resolution on various hosts we tried during the array API meeting.
comment,33212,,"Weird error unrelated to the lock-file update on macOS arm [build log](https://github.com/scikit-learn/scikit-learn/actions/runs/21715937167/job/62631917923). I restarted the CI to see if it disappears ...
```
__________ test_estimator_get_response_values[False-estimator2-score] __________
[gw1] darwin -- Python 3.13.11 /Users/runner/miniconda3/envs/testvenv/bin/python

estimator = KMeans(n_clusters=2, n_init=1), response_method = 'score'
return_response_method_used = False

    @pytest.mark.parametrize(
        ""estimator, response_method"",
        [
            (LinearRegression(), ""predict""),
            (KMeans(n_clusters=2, n_init=1), ""predict""),
            (KMeans(n_clusters=2, n_init=1), ""score""),
            (KMeans(n_clusters=2, n_init=1), [""predict"", ""score""]),
            (IsolationForest(random_state=0), ""predict""),
            (IsolationForest(random_state=0), ""decision_function""),
            (IsolationForest(random_state=0), [""decision_function"", ""predict""]),
        ],
    )
    @pytest.mark.parametrize(""return_response_method_used"", [True, False])
    def test_estimator_get_response_values(
        estimator, response_method, return_response_method_used
    ):
        """"""Check the behaviour of `_get_response_values`.""""""
        X, y = np.random.RandomState(0).randn(10, 2), np.array([0, 1] * 5)
        estimator.fit(X, y)
        results = _get_response_values(
            estimator,
            X,
            response_method=response_method,
            return_response_method_used=return_response_method_used,
        )
        chosen_response_method = (
            response_method[0] if isinstance(response_method, list) else response_method
        )
        prediction_method = getattr(estimator, chosen_response_method)
>       assert_array_equal(results[0], prediction_method(X))
E       AssertionError: 
E       Arrays are not equal
E       
E       Mismatched elements: 1 / 1 (100%)
E       Max absolute difference among violations: 8.8817842e-16
E       Max relative difference among violations: 1.36774746e-16
E        ACTUAL: array(-6.493731)
E        DESIRED: array(-6.493731)

X          = array([[ 1.76405235,  0.40015721],
       [ 0.97873798,  2.2408932 ],
       [ 1.86755799, -0.97727788],
       [ 0.95... 0.12167502],
       [ 0.44386323,  0.33367433],
       [ 1.49407907, -0.20515826],
       [ 0.3130677 , -0.85409574]])
chosen_response_method = 'score'
estimator  = KMeans(n_clusters=2, n_init=1)
prediction_method = <bound method _BaseKMeans.score of KMeans(n_clusters=2, n_init=1)>
response_method = 'score'
results    = (-6.493731071053089, None)
return_response_method_used = False
y          = array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])

../sklearn/utils/tests/test_response.py:81: AssertionError
```"
pull_request,33211,ENH add `xlim`/`ylim` parameters to DecisionBoundaryDisplay,"<!--
üôå Thanks for contributing a pull request!

üëÄ Please ensure you have taken a look at the contribution guidelines:
https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md

‚úÖ In particular following the pull request checklist will increase the likelihood
of having maintainers review your PR:
https://scikit-learn.org/dev/developers/contributing.html#pull-request-checklist

üìã If your PR is likely to affect users, you will need to add a changelog entry
describing your PR changes, see:
https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/README.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Towards https://github.com/scikit-learn/scikit-learn/issues/27462 (supersedes #31693 )

#### What does this implement/fix? Explain your changes.
Add `xlim` and `ylim` parameters to `from_estimator` and `plot` methods to allow setting the boundaries for the plot. Also added corresponding tests.

See the discussion in #31693 for the reason of extending the grid first (instead of simply overwriting the boundaries).

@lucyleeow @ogrisel @StefanieSenger 
#### AI usage disclosure
<!--
If AI tools were involved in creating this PR, please check all boxes that apply
below and make sure that you adhere to our Automated Contributions Policy:
https://scikit-learn.org/dev/developers/contributing.html#automated-contributions-policy
-->
I used AI assistance for:
- [ ] Code generation (e.g., when writing an implementation or fixing a bug)
- [x] Test/benchmark generation
- [ ] Documentation (including examples)
- [ ] Research and understanding


#### Any other comments?


<!--
Thank you for your patience. Changes to scikit-learn require careful
attention, but with limited maintainer time, not every contribution can be reviewed
quickly.
For more information and tips on improving your pull request, see:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
"
comment,33211,,"Thank you for the feedback! 

>Is it possible to add a test for when xlim and ylim 'shrink' the grid, we infer n_classes thus checking that the colors remain consistent?

Good point, I'll try to come up with one.

>I am not sure how we could get the best of both words: stable colors across zoom levels while preserving the same resolution at any zoom level (without computing gigabytes of unrendered response values).

I don't see a simple solution right away, but `grid_resolution` can be set by the user in `from_estimator`. Maybe we could add a note there that it should be increased when zooming in?"
comment,33211,,"> I don't see a simple solution right away, but grid_resolution can be set by the user in from_estimator. Maybe we could add a note there that it should be increased when zooming in?

Since this is only for purpose of inferring `n_classes` when using `type_of_target` what if we use the original `X` instead of the grid, as that should capture all classes/labels right? e.g.:

```diff
@@ -589,11 +589,19 @@ class DecisionBoundaryDisplay:
         elif is_clusterer(estimator) and hasattr(estimator, ""labels_""):
             n_classes = len(np.unique(estimator.labels_))
         else:
-            target_type = type_of_target(response)
+            # Use original X (not grid) to infer number of classes/labels, ensuring all
+            # classes/labels are captured even if some fall outside the grid boundaries
+            response_all, _ = _get_response_values(
+                estimator,
+                X,
+                response_method=prediction_method,
+                pos_label=class_of_interest,
+            )
+            target_type = type_of_target(response_all)
             if target_type in (""binary"", ""continuous""):
                 n_classes = 2
             elif target_type == ""multiclass"":
-                n_classes = len(np.unique(response))
+                n_classes = len(np.unique(response_all))
             else:
                 raise ValueError(
                     ""Number of classes or labels cannot be inferred from ""
```

"
comment,33211,,">Since this is only for purpose of inferring n_classes when using type_of_target what if we use the original X instead of the grid, as that should capture all classes/labels right?

That sounds reasonable. Especially since we might need to calculate these again anyway for the scatter plot integration planned in https://github.com/scikit-learn/scikit-learn/issues/33094
I'll adapt the code."
comment,33211,,"It turns out to be a bit more complex again: Since we currently just store the number of classes, the colors are not consistent when zooming in, because `plot` just selects the first ones from the colormap as it doesn't know about the order of the classes. I think we also need to first map the classes to their indices and select the colors based on that. This makes things a bit more complicated, but I think it will also be necessary to ensure consistency for the scatterplot, so I think it is worth the effort."
comment,33211,,"Ah, actually, the `LabelEncoder` takes care of the mapping (at least for classifiers), so it is actually only a matter of selecting colors according to the response values in `plot`. For `predict`, this is straight forward, for the others it should be the same conceptually, I'll look into that next."
comment,33211,,"In the end it was even easier for the `non-predict` `contourf` and `pcolormesh` versions, as the enumeration already takes care of the color selection. 

Note: This PR does not address the following cases (which will be addressed in their own PRs): 
* Matching colors of scatterplot for clusterers (see [#33094](https://github.com/scikit-learn/scikit-learn/issues/#33094) and [#33119](https://github.com/scikit-learn/scikit-learn/issues/#33119))
* The `contour` case (see [#33108](https://github.com/scikit-learn/scikit-learn/issues/#33108))

I will still look into adding an explicit test for the color consistency when zooming in."
comment,33211,,"New can of worms: 
For the binary cases (binary classification/clustering, outlier-detection and regression (as we decided to treat it like binary `decision_function`)), we don't handle the color selection (see Point 4 in https://github.com/scikit-learn/scikit-learn/issues/33115), so there is currently no easy way of making the colors stay consistent when zooming in in these cases.

I don't want to mix these things up, so I think it makes sense to pause this PR until I tackled [#33115](https://github.com/scikit-learn/scikit-learn/issues/33115) and [#33094](https://github.com/scikit-learn/scikit-learn/issues/33094), then I can check if @lucyleeow's suggestion can be integrated more easily to solve the pixel issue."
pull_request,33210,FIX remove redundant yield of `check_estimator_cloneable`,"#### Reference Issues/PRs
noticed while reviewing #33197

#### What does this implement/fix? Explain your changes.
Removes a redundant yield of `check_estimator_cloneable`, which had accidentally been introduced in #29832.
`check_estimator_cloneable` is always yielded first in `estimator_checks_generator` and `_yield_all_checks` (which was also yielding it) is exclusively used in there.

https://github.com/scikit-learn/scikit-learn/blob/d3898d9d57aeb1e960d266613a2e31b07bca39d7/sklearn/utils/estimator_checks.py#L566-L568

#### AI usage disclosure
<!--
If AI tools were involved in creating this PR, please check all boxes that apply
below and make sure that you adhere to our Automated Contributions Policy:
https://scikit-learn.org/dev/developers/contributing.html#automated-contributions-policy
-->
I used AI assistance for:
- [ ] Code generation (e.g., when writing an implementation or fixing a bug)
- [ ] Test/benchmark generation
- [ ] Documentation (including examples)
- [x] Research and understanding"
pull_request,33209,Fix LogisticRegressionCV scoring when CV folds miss class labels,"fixes #33207
`LogisticRegressionCV` can fail with probabilistic scorers such as `neg_brier_score` and `neg_log_loss` when a cross-validation test fold does not contain all class labels.

The failure occurs because `_log_reg_scoring_path` calls the scorer without passing the full set of class labels. In this case, `y_true` has fewer classes than the predicted probability matrix, causing a shape mismatch inside the scorer.

Changes
- Update `_log_reg_scoring_path` to pass `labels=classes` to scorers that support it.
- For built-in sklearn scorers, inject `labels` into a shallow copy of the scorer‚Äôs internal keyword arguments to avoid mutating user objects.
- For custom callable scorers, pass `labels` only if explicitly supported by the callable signature.
- Add a regression test covering missing-class folds for probabilistic scorers.
### Result
Prevents crashes when CV folds lack class labels.
Preserves existing scorer behavior and state.
Resolves the `FIXME` noted in` _log_reg_scoring_path`.

### Tests
-  sklearn/linear_model: 2,346 passed
- sklearn/metrics: 3,083 passed
- New Regression Test: Verified & passed"
comment,33209,,"This looks like a legitimate problem that needs solving. Pinging @lorentzenchr because `git blame` suggests you left the FIXME comment. Also @adrinjalali because it seems like there is some interplay with metadata routing.

I'm not sure if the proposed fix is the right thing to do, it seems quite complicated and like it reaches quite deep into the ""inners"" of scorers. While exploring alternatives I thought that manually (via `pdb.set_trace()`) modifying `score_params` to include the labels would be a way to fix this. However when I do that I get an error related to metadata routing:
```
ValueError: Passing extra keyword arguments to make_scorer(brier_score_loss, greater_is_better=False, response_method='predict_proba') is only supported if enable_metadata_routing=True, which you can set using `sklearn.set_config`. See the User Guide <https://scikit-learn.org/stable/metadata_routing.html> for more details. Extra parameters passed are: {'labels'}
```

I'm not sure what to do about that. To me it seems like adding a `""labels""` entry to `score_params` would be the right thing to do. It would use existing infrastructure for passing around additional keyword arguments.

However, maybe Christian has some thoughts on how to solve this that he remembers from when he wrote the FIXME comment.

@un1u3 thanks for reporting this and proposing a fix. For the moment I'd wait with making more changes until we hear from the above two maintainers."
comment,33209,,"Duplicate of https://github.com/scikit-learn/scikit-learn/pull/32828 and the CI fails.

However, this might give an idea for the other PR."
pull_request,33208,Fix LogisticRegressionCV Brier scoring when CV folds lack classes,"fixes #33207
NOTE: Closed this PR

Why
`LogisticRegressionCV` fails with `scoring=""neg_brier_score""` when a CV test fold does not contain all class labels. In multiclass settings, `brier_score_loss` requires explicit `labels` for probability arrays. A `FIXME` in `_log_reg_scoring_path `already documents this exact failure mode. This PR makes the built-in scorer robust without requiring users to define a custom scorer.

What
Detect the Brier scorer in `_log_reg_scoring_path` and inject `labels=classes` into the scorer‚Äôs kwargs when missing.

Update the Iris CV test to use `scoring=""neg_brier_score""` and adjust expectations to the valid multiclass Brier score range.

testing 
```python
33377 passed,
9288 skipped, 
147 xfailed, 
66 xpassed, 
3880 warnings
```
Note
- No public API changes.
- Change is limited to internal handling of the Brier scorer.
"
issue,33207,"LogisticRegressionCV with scoring = ""neg_brier_score"" fails when a CV test split is missing a class.","`neg_brier_score` uses `brier_score_loss`, which requires `labels` when `y_true `does not include all classes. In `LogisticRegressionCV`, the scorer is called without `labels`, even though the estimator is trained on all classes and probabilities are emitted for all classes. This causes a `ValueError `if any test fold misses a class. There is already a `FIXME`in the source noting exactly this. The scoring should pass `labels=classes `(or equivalent) to be robust.

<img width=""965"" height=""269"" alt=""Image"" src=""https://github.com/user-attachments/assets/1c5444b5-753d-46c7-8d17-a9ecf7318ee7"" />

### Minimal Reproduction 
```python 
import numpy as np
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegressionCV
import sklearn

print(""sklearn"", sklearn.__version__)

X, y = make_classification(
    n_samples=90,
    n_features=8,
    n_informative=6,
    n_redundant=0,
    n_classes=3,
    n_clusters_per_class=1,
    random_state=0,
)

idx0 = np.flatnonzero(y == 0)
idx1 = np.flatnonzero(y == 1)
idx2 = np.flatnonzero(y == 2)

# Fold 0 test set has only classes 0 and 1
test0 = np.concatenate([idx0[:10], idx1[:10]])
train0 = np.setdiff1d(np.arange(len(y)), test0)

# Fold 1 test set has only class 2
test1 = idx2[:10]
train1 = np.setdiff1d(np.arange(len(y)), test1)

cv = [(train0, test0), (train1, test1)]

clf = LogisticRegressionCV(
    cv=cv,
    scoring=""neg_brier_score"",
    solver=""lbfgs"",
    max_iter=200,
)

clf.fit(X, y)

```

### Observed Error 
```python
ValueError: y_true and y_prob contain different number of classes: 2 vs 3. 
Please provide the true labels explicitly through the labels argument. Classes found in y_true: [0 1]


```


### Suggest Fix (also mentioned in comments by a contributer)
When scoring is `neg_brier_score`, pass `labels=classes` via `score_params` or directly to the scorer call so the full class set is used"
comment,33207,,"I've reproduced the issue and identified the root cause. The problem occurs in _log_reg_scoring_path() when using neg_brier_score or similar metrics that rely on brier_score_loss. When a CV test split is missing a class, the scorer needs explicit labels parameter to know about all classes that the model was trained on.

Proposed fix: In _log_reg_scoring_path(), when the scoring method is ""neg_brier_score"" or ""brier_score_loss"", we should pass labels=classes to the scorer call. This ensures the Brier score calculation considers all classes, even if they're not present in the test fold.

The fix is minimal and addresses the exact scenario described in the FIXME comment and issue #33207 that's already in the code. This will make LogisticRegressionCV robust to class imbalance in cross-validation folds when using Brier score-based metrics."
comment,33207,,"> I've reproduced the issue and identified the root cause. The problem occurs in _log_reg_scoring_path() when using neg_brier_score or similar metrics that rely on brier_score_loss. When a CV test split is missing a class, the scorer needs explicit labels parameter to know about all classes that the model was trained on.
> 
> Proposed fix: In _log_reg_scoring_path(), when the scoring method is ""neg_brier_score"" or ""brier_score_loss"", we should pass labels=classes to the scorer call. This ensures the Brier score calculation considers all classes, even if they're not present in the test fold.
> 
> The fix is minimal and addresses the exact scenario described in the FIXME comment and issue [#33207](https://github.com/scikit-learn/scikit-learn/issues/33207) that's already in the code. This will make LogisticRegressionCV robust to class imbalance in cross-validation folds when using Brier score-based metrics.

the implementation checks whether the scorer supports a labels parmeter before passing it. This fixes `neg_brier_score` as noted in  `FIXME`, and also covers closely related probabilistic scorers (like `neg_log_loss`) that rely on full class information when some classes are absent from a CV fold.

For built-in sklearn `scorers`, `labels` is added to a shallow copy of the scorer‚Äôs parameters to avoid mutating user-provided objects and to respect existing metadata routing behavior. No user-facing configuration is required
"
comment,33207,,"Thank you.
Implemented your exact approach"
comment,33207,,Duplicate of https://github.com/scikit-learn/scikit-learn/issues/15389
issue,33206,TimeSeries Split: Allow for larger test_size than n_samples // (n_splits + 1),"### Describe the workflow you want to enable

I have a time series y` `with sequence length 1800. Each time stamp is equivalent to 2 seconds, meaning the whole time series covers a time span of 3600 seconds or 1 hour.  
I want to use the first 5 minutes/ 300 seconds/150 time stamps to predict the remaining time series.
Therefore, I want to do a 3-fold time-series cross-validation during the training of the forecasting model. This means that in each split, I take the first 50 time stamps to train the model, and the next 1650 time stamps to validate the model. In the third fold, I then use 150 time stamps (5 minutes) and predict the remaining 1650 time stamps in my series.
When creating the splitter with 
```
splitter = TimeSeriesSplit(n_splits=3, test_size=int(1800-seq_len))
split = splitter.split(y[0, :])  
```
When looping through the split with
```
for train_index, test_index in split
    train_indexes.append(train_index
    test_indexes.append(test_index) 
```
I get the error:
`ValueError: Too many splits=3 for number of samples=1800 with test_size=1648 and gap=0.`
which is due to the maximum test_size of `n_samples // (n_splits + 1)`.
I wonder why it is not possible to have much larger test sets comparred to train sets?

### Describe your proposed solution

I could, of course, write my own splitter, but I think it would be a good improvement to have this capability also in the `sklearn` framework to make it more complete.
Introducing a gap to only forecast the end of the time series is not an option for me due to the nature of my forecasting algorithm.

### Describe alternatives you've considered, if relevant

_No response_

### Additional context

_No response_"
comment,33206,,"Thanks for opening this feature request. In general we are not adding a lot of time series specific tooling to scikit-learn and instead recommending to use existing libraries that are compatible with scikit-learn. The reason for this is that time series forecasting is a big field where things are quite different from how most of scikit-learn works (e.g. there is no meaning in the order of samples).

For example https://tscv.readthedocs.io/en/latest/tutorial/roll_forward.html looks similar to what you are looking for, but not perfect. Maybe that library is a better place for this than scikit-learn"
issue,33205,Use `xp.linalg.pinv` instead of `xp.linalg.inv` in PCA,"As part of https://github.com/scikit-learn/scikit-learn/pull/33175 we noticed that the array API compliance test for `PCA` fails, but only when using a CI runner with CUDA and then using float32 and the CPU device. For now the test has been xfailed.

```
FAILED conda/envs/sklearn/lib/python3.13/site-packages/sklearn/tests/test_common.py::test_estimators[PCA()-check_array_api_input(array_namespace=torch,dtype_name=float32,device=cpu)] - torch._C._LinAlgError: linalg.inv: The diagonal element 10 is zero, the inversion could not be completed because the input matrix is singular.
```

<details><summary>Full traceback</summary>
<p>

```
_ test_estimators[PCA()-check_array_api_input(array_namespace=torch,dtype_name=float32,device=cpu)] _

estimator = PCA()
check = functools.partial(<function check_array_api_input at 0x7c055c520680>, 'PCA', array_namespace='torch', dtype_name='float32', device='cpu')
request = <FixtureRequest for <Function test_estimators[PCA()-check_array_api_input(array_namespace=torch,dtype_name=float32,device=cpu)]>>

    @parametrize_with_checks(
        list(_tested_estimators()), expected_failed_checks=_get_expected_failed_checks
    )
    def test_estimators(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(FutureWarning, ConvergenceWarning, UserWarning, LinAlgWarning)
        ):
>           check(estimator)

check      = functools.partial(<function check_array_api_input at 0x7c055c520680>, 'PCA', array_namespace='torch', dtype_name='float32', device='cpu')
estimator  = PCA()
request    = <FixtureRequest for <Function test_estimators[PCA()-check_array_api_input(array_namespace=torch,dtype_name=float32,device=cpu)]>>

conda/envs/sklearn/lib/python3.13/site-packages/sklearn/tests/test_common.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
conda/envs/sklearn/lib/python3.13/site-packages/sklearn/utils/estimator_checks.py:1201: in check_array_api_input
    result_xp = getattr(est_xp, method_name)(X_xp, y_xp)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        X          = array([[ 1.0830512 , -1.1429703 ,  0.05820872,  0.5607845 ,  1.053802  ,
        -0.5948778 ,  0.35778737,  0.5275467 ....35711256, -0.5182702 ,
         0.4970482 , -0.21967189, -1.115722  ,  2.357159  , -1.3852317 ]],
      dtype=float32)
        X_xp       = tensor([[ 1.0831, -1.1430,  0.0582,  0.5608,  1.0538, -0.5949,  0.3578,  0.5275,
          0.1216, -0.6640],
        [... 1.0552],
        [ 1.4779, -1.9876,  0.0918,  0.3571, -0.5183,  0.4970, -0.2197, -1.1157,
          2.3572, -1.3852]])
        array_attributes = {'components_': array([[-2.5761431e-02,  5.4883957e-03, -6.8862990e-02, -5.3921595e-02,
         7.0448972e-02,  3.047...7408 ,  0.44903848,
        0.12485652, -0.24021086, -0.1826419 ,  0.23647286, -0.06734962],
      dtype=float32), ...}
        array_namespace = 'torch'
        attribute  = array([1.1475833e+01, 8.3189106e+00, 6.3970122e+00, 5.9165435e+00,
       5.6904674e+00, 4.1923413e+00, 3.8798978e+00, 3.3633292e+00,
       8.8419966e-07, 6.1141361e-07], dtype=float32)
        attribute_ns = 'sklearn.externals.array_api_compat.torch'
        check_sample_weight = False
        check_values = False
        device     = 'cpu'
        dtype_name = 'float32'
        est        = PCA(random_state=0)
        est_fitted_with_as_array = PCA(random_state=0)
        est_xp     = PCA(random_state=0)
        est_xp_param = tensor([1.1476e+01, 8.3189e+00, 6.3970e+00, 5.9165e+00, 5.6905e+00, 4.1923e+00,
        3.8799e+00, 3.3633e+00, 9.0832e-07, 4.8586e-07])
        est_xp_param_np = array([1.1475831e+01, 8.3189087e+00, 6.3970146e+00, 5.9165444e+00,
       5.6904664e+00, 4.1923423e+00, 3.8798997e+00, 3.3633294e+00,
       9.0831952e-07, 4.8586008e-07], dtype=float32)
        estimator_orig = PCA()
        expect_only_array_outputs = True
        fit_kwargs = {}
        fit_kwargs_xp = {}
        input_ns   = 'sklearn.externals.array_api_compat.torch'
        key        = 'singular_values_'
        method     = <bound method PCA.score of PCA(random_state=0)>
        method_name = 'score'
        methods    = ('score', 'score_samples', 'decision_function', 'predict', 'predict_log_proba', 'predict_proba', ...)
        name       = 'PCA'
        numpy_asarray_works = True
        result     = -inf
        xp         = <module 'sklearn.externals.array_api_compat.torch' from '/home/runner/conda/envs/sklearn/lib/python3.13/site-packages/sklearn/externals/array_api_compat/torch/__init__.py'>
        y          = array([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
       1, 0, 1, 0, 1, 0, 1, 0])
        y_xp       = tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0])
conda/envs/sklearn/lib/python3.13/site-packages/sklearn/decomposition/_pca.py:838: in score
    return float(xp.mean(self.score_samples(X)))
                         ^^^^^^^^^^^^^^^^^^^^^
        X          = tensor([[ 1.0831, -1.1430,  0.0582,  0.5608,  1.0538, -0.5949,  0.3578,  0.5275,
          0.1216, -0.6640],
        [... 1.0552],
        [ 1.4779, -1.9876,  0.0918,  0.3571, -0.5183,  0.4970, -0.2197, -1.1157,
          2.3572, -1.3852]])
        _          = True
        self       = PCA(random_state=0)
        xp         = <module 'sklearn.externals.array_api_compat.torch' from '/home/runner/conda/envs/sklearn/lib/python3.13/site-packages/sklearn/externals/array_api_compat/torch/__init__.py'>
        y          = tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0])
conda/envs/sklearn/lib/python3.13/site-packages/sklearn/decomposition/_pca.py:812: in score_samples
    precision = self.get_precision()
                ^^^^^^^^^^^^^^^^^^^^
        X          = tensor([[ 1.0831, -1.1430,  0.0582,  0.5608,  1.0538, -0.5949,  0.3578,  0.5275,
          0.1216, -0.6640],
        [... 1.0552],
        [ 1.4779, -1.9876,  0.0918,  0.3571, -0.5183,  0.4970, -0.2197, -1.1157,
          2.3572, -1.3852]])
        Xr         = tensor([[ 1.0999e+00, -9.2279e-01, -1.1772e-01,  4.1604e-01,  6.0476e-01,
         -7.1973e-01,  5.9800e-01,  7.1019e-...+00, -8.4169e-02,  2.1237e-01, -9.6731e-01,
          3.7219e-01,  2.0539e-02, -9.3308e-01,  2.1207e+00, -1.3179e+00]])
        _          = True
        n_features = 10
        self       = PCA(random_state=0)
        xp         = <module 'sklearn.externals.array_api_compat.torch' from '/home/runner/conda/envs/sklearn/lib/python3.13/site-packages/sklearn/externals/array_api_compat/torch/__init__.py'>
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = PCA(random_state=0)

    def get_precision(self):
        """"""Compute data precision matrix with the generative model.
    
        Equals the inverse of the covariance but computed with
        the matrix inversion lemma for efficiency.
    
        Returns
        -------
        precision : array, shape=(n_features, n_features)
            Estimated precision of data.
        """"""
        xp, is_array_api_compliant = get_namespace(self.components_)
    
        n_features = self.components_.shape[1]
    
        # handle corner cases first
        if self.n_components_ == 0:
            return xp.eye(n_features) / self.noise_variance_
    
        if is_array_api_compliant:
            linalg_inv = xp.linalg.inv
        else:
            linalg_inv = linalg.inv
    
        if self.noise_variance_ == 0.0:
>           return linalg_inv(self.get_covariance())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           torch._C._LinAlgError: linalg.inv: The diagonal element 10 is zero, the inversion could not be completed because the input matrix is singular.

is_array_api_compliant = True
linalg_inv = <built-in function linalg_inv>
n_features = 10
self       = PCA(random_state=0)
xp         = <module 'sklearn.externals.array_api_compat.torch' from '/home/runner/conda/envs/sklearn/lib/python3.13/site-packages/sklearn/externals/array_api_compat/torch/__init__.py'>

conda/envs/sklearn/lib/python3.13/site-packages/sklearn/decomposition/_base.py:82: _LinAlgError
```

</p>
</details> 

https://github.com/scikit-learn/scikit-learn/pull/33175#issuecomment-3834360881 suggests a possible fix. As part of making the change it is important to investigate why this happens. The problem showed up when we upgraded the version of PyTorch.

cc @ogrisel "
comment,33205,,"> As part of making the change it is important to investigate why this happens. 

and also why it does not happen with NumPy.

If we find out that `self.get_covariance()` can return rank deficient matrices under some conditions (which?), would using a pseudo inverse be suitable to estimate the precision matrix? Or is it better to fail explicitly in this case?"
comment,33205,,"Hi @betatim @ogrisel, I would like to work on this issue.

I can investigate why the covariance matrix becomes singular with PyTorch/CPU and implement the switch to xp.linalg.pinv as suggested. I'll also check if this impacts the precision matrix calculation as discussed."
pull_request,33204,DOC: Fix typo in API reference description,"Fixes a small typo in the API reference configuration documentation.
No functional changes.
"
comment,33204,,"@StefanieSenger 
Thanks so much for the review! üòä"
pull_request,33203,CI Have the name of the failing build in the tracking issue,"<!--
üôå Thanks for contributing a pull request!

üëÄ Please ensure you have taken a look at the contribution guidelines:
https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md

‚úÖ In particular following the pull request checklist will increase the likelihood
of having maintainers review your PR:
https://scikit-learn.org/dev/developers/contributing.html#pull-request-checklist

üìã If your PR is likely to affect users, you will need to add a changelog entry
describing your PR changes, see:
https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/README.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fix issue raised in https://github.com/scikit-learn/scikit-learn/issues/33152#issuecomment-3833793227

#### What does this implement/fix? Explain your changes.

Get the name of the failing build in the tracking issue name, even if the build is not in the job matrix.

#### AI usage disclosure
<!--
If AI tools were involved in creating this PR, please check all boxes that apply
below and make sure that you adhere to our Automated Contributions Policy:
https://scikit-learn.org/dev/developers/contributing.html#automated-contributions-policy
-->
I used AI assistance for:
- [ ] Code generation (e.g., when writing an implementation or fixing a bug)
- [ ] Test/benchmark generation
- [ ] Documentation (including examples)
- [x] Research and understanding


#### Any other comments?

Apparently there is no way to access the job name from the job steps, so I used an env variable containing the job name as a workaround. 

<!--
Thank you for your patience. Changes to scikit-learn require careful
attention, but with limited maintainer time, not every contribution can be reviewed
quickly.
For more information and tips on improving your pull request, see:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
"
comment,33203,,"Nice, I can tell from your tests on your fork this seems to work fine, see e.g. https://github.com/FrancoisPgm/scikit-learn/issues/46."
pull_request,33202,FIX `n_classes` in DecisionBoundaryDisplay with custom estimators,"<!--
üôå Thanks for contributing a pull request!

üëÄ Please ensure you have taken a look at the contribution guidelines:
https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md

‚úÖ In particular following the pull request checklist will increase the likelihood
of having maintainers review your PR:
https://scikit-learn.org/dev/developers/contributing.html#pull-request-checklist

üìã If your PR is likely to affect users, you will need to add a changelog entry
describing your PR changes, see:
https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/README.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes https://github.com/scikit-learn/scikit-learn/issues/33194

#### What does this implement/fix? Explain your changes.
Adds inferring `n_classes` from `type_of_target` and raises an error if it still can't be inferred.

#### AI usage disclosure
<!--
If AI tools were involved in creating this PR, please check all boxes that apply
below and make sure that you adhere to our Automated Contributions Policy:
https://scikit-learn.org/dev/developers/contributing.html#automated-contributions-policy
-->
I used AI assistance for:
- [ ] Code generation (e.g., when writing an implementation or fixing a bug)
- [x] Test/benchmark generation
- [ ] Documentation (including examples)
- [ ] Research and understanding


#### Any other comments?
I also adapted the example to show the correct colormap (as `viridis` is no longer the default).

<!--
Thank you for your patience. Changes to scikit-learn require careful
attention, but with limited maintainer time, not every contribution can be reviewed
quickly.
For more information and tips on improving your pull request, see:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention. 

Thanks for contributing!
-->
"
comment,33202,,I'll look into custom estimators a bit more to make codecov happy.
comment,33202,,I think this is ready for review now @lucyleeow @ogrisel
pull_request,33201,Teaching pull request - VCS Class - G21,"<!--
üôå Thanks for contributing a pull request!

üëÄ Please ensure you have taken a look at the contribution guidelines:
https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md

‚úÖ In particular following the pull request checklist will increase the likelihood
of having maintainers review your PR:
https://scikit-learn.org/dev/developers/contributing.html#pull-request-checklist

üìã If your PR is likely to affect users, you will need to add a changelog entry
describing your PR changes, see:
https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/README.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.


#### AI usage disclosure
<!--
If AI tools were involved in creating this PR, please check all boxes that apply
below and make sure that you adhere to our Automated Contributions Policy:
https://scikit-learn.org/dev/developers/contributing.html#automated-contributions-policy
-->
I used AI assistance for:
- [ ] Code generation (e.g., when writing an implementation or fixing a bug)
- [ ] Test/benchmark generation
- [ ] Documentation (including examples)
- [ ] Research and understanding


#### Any other comments?


<!--
Thank you for your patience. Changes to scikit-learn require careful
attention, but with limited maintainer time, not every contribution can be reviewed
quickly.
For more information and tips on improving your pull request, see:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
"
pull_request,33200,FEA Add array API support to `roc_auc_score`,"#### Reference Issues/PRs

Towards #26024.


#### What does this implement/fix? Explain your changes.

This PR adds array API support to [<code>roc_auc_score</code>](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html).


#### AI usage disclosure

I used AI assistance for:
- [x] Research and understanding


#### Any other comments?

Waiting for CI to turn green."
comment,33200,,"Currently, we are running into an issue with mixed-type promotion in `array-api-strict`, as reported in #32552:

```bash
TypeError: array_api_strict.float64 and array_api_strict.int64 cannot be type promoted together
```

I'll investigate this further later."
pull_request,33199,FIX Exclude all-zero relevance samples from NDCG computation (#29521),"## Description

Fixes #29521 

When all ground truth relevances are zero for a sample, both DCG and IDCG are 0, making NDCG undefined (0/0). Previously, such samples were assigned a score of 0 and included in averaging, causing `ndcg_score(y, y)` to return values less than 1.0.

According to the original 2002 Jarvelin & Kekalainen paper that introduced NDCG:
> The (D)CG vectors for each IR technique can be normalized by dividing them by the corresponding ideal (D)CG vectors, component by component. In this way, for any vector position, the normalized value 1 represents ideal performance...

This means `ndcg_score(y, y)` must always equal 1.0.

## Changes

**sklearn/metrics/_ranking.py:**
- Added detection for samples where all relevances are 0
- Raises `UserWarning` when such samples are encountered
- Excludes these samples from the averaging calculation
- Returns `np.nan` if all samples have all-zero relevances
- Updated docstring with `versionchanged` notes explaining the new behavior

**sklearn/metrics/tests/test_ranking.py:**
- Added `test_ndcg_all_zero_relevance()` with comprehensive test cases:
  - Mix of relevant and all-zero samples (should return 1.0)
  - All samples with all-zero relevances (should return NaN)
  - Single sample with all-zero relevance (should return NaN)
  - Normal case without all-zero samples (should return 1.0)

## Example

Before this fix:
```python
import numpy as np
from sklearn.metrics import ndcg_score

y = np.array([[1.0, 0.0, 1.0], [0.0, 0.0, 0.0]])
ndcg_score(y, y)  # Returns 0.5 (WRONG)
```

After this fix:
```python
import numpy as np
from sklearn.metrics import ndcg_score

y = np.array([[1.0, 0.0, 1.0], [0.0, 0.0, 0.0]])
# UserWarning: All ground truth relevances are zero for at least one sample...
ndcg_score(y, y)  # Returns 1.0 (CORRECT)
```"
pull_request,33198,TST: Use global_random_seed in test_dict_learning.py,"This PR contributes to issue #22827 by converting `sklearn/decomposition/tests/test_dict_learning.py` to use the `global_random_seed` fixture.

## Changes

- Added `rng_global` fixture that depends on `global_random_seed`
- Renamed the data fixture from `X` to `X_data` to avoid shadowing the module-level `X` array
- Restored deterministic module-level `X` for tests that rely on exact numerical values
- Updated tests to use `global_random_seed` for improved reproducibility
- Fixed fixture scoping issues and removed duplicate parameterization
- Stabilized seed-sensitive tests by using fixed `random_state=0` where needed
- Updated error message regex to match current sklearn validation messages

## Testing

All 199 tests in the module pass successfully:
- Tested with default seed
- Verified key tests with representative seeds (0, 42, 99)
- Confirmed no regressions in test behavior

Addresses #22827 (one file at a time as recommended in the meta-issue)"
pull_request,33197,FIX Remove redundant yield of check_class_weight_balanced_linear_classifier,"Reference Issues/PRs
Fixes #33154.

The duplication was accidentally introduced in #29712. This PR implements the suggestion to remove the redundancy and ensure tests continue to pass efficiently.

What does this implement/fix? Explain your changes.
In sklearn/utils/estimator_checks.py, the function _yield_classifier_checks was yielding check_class_weight_balanced_linear_classifier twice for linear classifiers supporting the class_weight parameter.

This resulted in redundant test executions during common estimator checks (e.g., for LogisticRegression), wasting CI/CD time and creating duplicate log entries.

Changes:

Removed the duplicate if block in sklearn/utils/estimator_checks.py.
Verified that the check is now yielded exactly once for linear classifiers and correctly ignored for non-linear classifiers.
Added a changelog entry in doc/whats_new/upcoming_changes/sklearn.utils/. 

AI usage disclosure
I used AI assistance for:
 Code generation.
 Documentation (including examples)

Any other comments?
The fix was verified against several cases:

LogisticRegression (Linear + class_weight): Yields exactly 1 check (Verified).
Linear Classifier (No class_weight): Yields 0 checks (Verified).
Non-Linear Classifier: Yields 0 checks (Verified).
Linear with class_weight=None: Yields 1 check (Verified).
Maintainers: As this addresses duplication introduced in #29712, @ogrisel @adrinjalali @OmarManzoor @glemaitre "
comment,33197,,"Please don't ping people directly by @-mentioning them. Especially if your PR is not yet ready for people to review it.

Here are some things to do:
* fix the linting errors
* check that all CI jobs run successfully
* nicely format your Pull Request message (the headings in the template exist for a reason, use code blocks to format code, maintain the checklist that exist in the template

Once all that is done you may request a review from the author of https://github.com/scikit-learn/scikit-learn/pull/29712 (not several maintainers).

The reason fixing formatting and linter issues is important is that you want to present your work in the best way possible. Messy, unclear, hard to read PRs have a lower probability of being reviewed. This is not because people don't like you or the work but because there are many, many PRs in need of review. After filtering on simple things like ""first impression is that this is messy"" there are still many PRs left to review. This means investing time in the presentation is a simple thing to do to increase the chances of receiving a review."
comment,33197,,"I don't think we need a changelog for such a change, so I pushed a commit removing it and enabled auto-merge."
pull_request,33196,TST: Add common test for transform() on different sparse formats,"#### Reference Issues/PRs
Fixes #14176

#### What does this implement/fix? Explain your changes.
This PR extends the common estimator test `_check_estimator_sparse_container` to test the `transform()` method on different sparse matrix formats (CSR, CSC, COO, LIL, DOK, DIA, BSR), similar to how it already tests `predict()` and `predict_proba()`.

The change adds a simple check that:
1. Verifies the estimator has a `transform` method
2. Calls `transform()` on sparse matrices
3. Asserts the output has the correct number of samples

This ensures all transformers in scikit-learn are tested with various sparse formats, making individual test files like `test_truncated_svd.py::test_sparse_formats` redundant.

#### AI usage disclosure
I used AI assistance for:
- [x] Code generation (e.g., when writing an implementation or fixing a bug)
- [ ] Test/benchmark generation
- [ ] Documentation (including examples)
- [x] Research and understanding"
pull_request,33195,DOC Add academic references to common pitfalls guide,"#### Reference Issues/PRs
Fixes #24287

#### What does this implement/fix?
This PR adds a comprehensive ""References"" section to the Common Pitfalls guide with peer-reviewed academic publications that support the recommended practices.

#### What are the changes?
Added references for:
- **Data Splitting**: Xu & Goodacre (2018) - comparative study of CV, bootstrap, and systematic sampling
- **Data Leakage**: Kaufman et al. (2012) - formulation, detection, and avoidance
- **Model Selection**: Cawley & Talbot (2010) - over-fitting and selection bias
- **Feature Selection**: Ambroise & McLachlan (2002) - selection bias in gene extraction
- **General Best Practices**: Hastie et al. (2009) and Bishop (2006)

#### Any other comments?
The guide currently presents best practices without citations. Adding these authoritative academic references:
- Strengthens the scientific foundation of the recommendations
- Provides users with sources for deeper understanding
- Establishes that these are evidence-based practices, not just opinions

This is particularly important since scikit-learn is used for serious work in both industry and academia."
comment,33195,,"I would rather not have a single section with a set of generic references for the full guide but instead, add individual relevant references that support each individual pitfalls and recommended remediation to each individual subsection."
comment,33195,,"@ogrisel Thank you for the feedback! I've updated the PR to integrate the references inline within each relevant subsection rather than having separate reference blocks.

Changes made:
- Removed all standalone ""References"" sections
- Added inline citations where each specific pitfall/recommendation is discussed:
  - Kaufman et al. (2012) on data leakage in the leakage subsection
  - Ambroise & McLachlan (2002) on feature selection bias in the feature selection subsection
  - Cawley & Talbot (2010) on model selection in the cross-validation subsection
- Added a proper References section at the end with the full citations using reST footnote format

Please let me know if this better aligns with what you had in mind!"
comment,33195,,"Hello @jaideepj2004 - thanks for your enthusiasm for contributing to scikit-learn. You opened many PRs in a very short time frame. They reference very old issues. We welcome well thought out contributions that target problems real humans are experiencing when using scikit-learn for real world applications. Mass PR creation often indicates the opposite of this.

The fact that your profile shows many new PRs in a short time, you do not follow the PR template (missing AI deceleration), target old issues, your messages look similar to those created by AI agents, etc makes me think that you are going for breadth and not depth. Reviewing PRs is the limiting factor for projects like scikit-learn, as such we need to focus our efforts on high value contributions (ideally a small amount of review time and changes that effect many real world users). This means that it would be good for you to review the PRs that you opened simultaneously and close those that you do not think represent your top two contributions in term of real world impact.

Mass PRs by users with no contribution history or other connection to the project are typically a sign of spam/people trying to get GitHub ""karma"" without actually caring about improving the projects they are sending PRs to. This means we are very likely to mass close those PRs and potentially ban the user."
issue,33194,BUG undefined `n_classes` for custom estimators in `DecisionBoundaryDisplay`,"### Describe the bug and give evidence about its user-facing impact

When introducing `n_classes` for the `DecisionBoundaryDisplay` in  https://github.com/scikit-learn/scikit-learn/pull/33015, we didn't think about custom estimators like in [this example](https://scikit-learn.org/dev/auto_examples/cluster/plot_inductive_clustering.html) (re-created in the minimal example below), where `n_classes` can not be extracted from the estimator without knowing its architecture, which currently breaks [CircleCI](https://app.circleci.com/pipelines/github/scikit-learn/scikit-learn?branch=main).
(@lesteve, just so you know we're fixing it!)

### Steps/Code to Reproduce

```python
import matplotlib.pyplot as plt

from sklearn.base import BaseEstimator, clone
from sklearn.cluster import AgglomerativeClustering
from sklearn.datasets import make_blobs
from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import DecisionBoundaryDisplay
from sklearn.utils.validation import check_is_fitted

N_SAMPLES = 5000
RANDOM_STATE = 42

class InductiveClusterer(BaseEstimator):
    def __init__(self, clusterer, classifier):
        self.clusterer = clusterer
        self.classifier = classifier

    def fit(self, X, y=None):
        self.clusterer_ = clone(self.clusterer)
        self.classifier_ = clone(self.classifier)
        y = self.clusterer_.fit_predict(X)
        self.classifier_.fit(X, y)
        return self

    def predict(self, X):
        check_is_fitted(self)
        return self.classifier_.predict(X)

# Generate some training data from clustering
X, y = make_blobs(
    n_samples=N_SAMPLES,
    cluster_std=[1.0, 1.0, 0.5],
    centers=[(-5, -5), (0, 0), (5, 5)],
    random_state=RANDOM_STATE,
)

# Train a clustering algorithm on the training data and get the cluster labels
clusterer = AgglomerativeClustering(n_clusters=3)
classifier = RandomForestClassifier(random_state=RANDOM_STATE)
inductive_learner = InductiveClusterer(clusterer, classifier).fit(X)

# Plotting decision regions
DecisionBoundaryDisplay.from_estimator(
    inductive_learner, X, response_method=""predict"", alpha=0.4
)
plt.show()
```

### Expected Results

Plot is shown.

### Actual Results

`UnboundLocalError: cannot access local variable 'n_classes' where it is not associated with a value`

### Versions

```shell
sklearn: 1.9.dev0
```

### Interest in fixing the bug

I don't think there is a sensible default value we could assume in these cases. My suggestion would be to add `n_classes=None` as a default parameter to `from_estimator` and raise a `ValueError` if it cannot be inferred from the estimator and has not been specified.
If you agree @ogrisel, @lucyleeow, I'll open a PR for this."
comment,33194,,"I'm not sure what the best option is. Looking at the `is_regressor` / `is_clusterer` etc functions - they are just checking for the presence of the relevant estimator tag.

Looking at our docs for [developing your own estimator](https://scikit-learn.org/stable/developers/develop.html#rolling-your-own-estimator) we do have a section suggesting use of estimator [tags](https://scikit-learn.org/stable/developers/develop.html#estimator-tags) (though of course this can't be relied on). There is also a section recommending use of our mixins (and ducktyping for checking type of estimator):

> We tend to use ‚Äúduck typing‚Äù instead of checking for [isinstance](https://docs.python.org/3/library/functions.html#isinstance), which means it‚Äôs technically possible to implement an estimator without inheriting from scikit-learn classes. However, if you don‚Äôt inherit from the right mixins, either there will be a large amount of boilerplate code for you to implement and keep in sync with scikit-learn development, or your estimator might not function the same way as a scikit-learn estimator.

I think we could do some more inferring before we error - here are some suggestions (not mutually exclusive):

* add the appropriate estimator tag to the estimator, in the example - this only fixes the example of course, but it is probably nice to demonstrate use of tags (which is what we recommend) in our own examples
* we could add ducktyping i.e. `is_regressor or isinstance(estimator, RegressorMixin)` - this will pick up more cases, but again not all
* add an `else` condition (to the logic where we infer `n_classes` in `from_estimator`) where we do `type_of_target` on `y` and infer type and then use `unique` to get `n_classes` if relevant - `type_of_target` is not always accurate but it gives reasonable results

@lesteve and @ogrisel may have more suggestions / thoughts to add though.
"
comment,33194,,"Since it is breaking CI, I would be in favour of a quick solution first.
I'll open a PR with a suggestion for adding an error and the missing test and a fix for the example.
I'm happy to iterate on the possible inferring options afterwards."
comment,33194,,"+1 for the first and last items suggested by @lucyleeow. I would rather not do `or isinstance(estimator, RegressorMixin)`: `is_regressor` should be enough to make the estimator type explicit."
comment,33194,,"> is_regressor should be enough to make the estimator type explicit.

Just a question as I am not familiar with user developed estimators - are you talking about sklearn defined estimators or user defined ones? If user defined ones, does this mean that they should add the appropriate estimator tags when developing their own estimator?"
comment,33194,,">If user defined ones, does this mean that they should add the appropriate estimator tags when developing their own estimator?

If I understand correctly, a user-defined estimator should also always inherit from the corresponding `Mixin` class (on top of the `BaseEstimator`), which will already have the correct `estimator_type` tag defined."
comment,33194,,">  should also always inherit from the corresponding Mixin class (on top of the BaseEstimator), which will already have the correct estimator_type tag defined.

Ah right, good point. Though as per our docs it is not dis-allowed:

> However, if you don‚Äôt inherit from the right mixins, either there will be a large amount of boilerplate code for you to implement and keep in sync with scikit-learn development, or your estimator might not function the same way as a scikit-learn estimator. Here we only document how to develop an estimator using our mixins. If you‚Äôre interested in implementing your estimator without inheriting from scikit-learn mixins, you‚Äôd need to check our implementations."
pull_request,33193,DOC Enhance density estimation documentation with ML connections,"#### Reference Issues/PRs
Fixes #15250

#### What does this implement/fix?
This PR enhances the density estimation documentation by adding a new section that explains the relationships between density estimation and other machine learning paradigms.

#### What are the changes?
Added ""Connections to Other Machine Learning Tasks"" section covering:
- **Unsupervised Learning**: How density estimation reveals data structure and enables anomaly detection
- **Feature Engineering**: Using density scores as features for supervised learning
- **Data Modeling**: Applications in sampling, imputation, and compression

Included authoritative academic references:
- Hastie, Tibshirani & Friedman (2009) - The Elements of Statistical Learning
- Bishop (2006) - Pattern Recognition and Machine Learning

#### Any other comments?
The original documentation mentioned these connections in passing but didn't explain them. This addition provides users with a clearer understanding of how density estimation fits into the broader ML landscape and provides references for deeper study."
comment,33193,,This is a PR for a very old issue that received very little attention over its lifetime. This means that the reporters problem did not resonate widely. As a result I think the way to go is to close the issue and this PR.
pull_request,33192,DOC Document BallTree's component-wise centroid calculation assumption,"#### Reference Issues/PRs
Fixes #12697

#### What does this implement/fix?
This PR documents that BallTree computes node centroids by averaging sample coordinates component-wise, which assumes such averaging produces valid points in the sample space.

#### Any other comments?
This assumption may not hold for custom metrics where samples live in non-Euclidean spaces or on manifolds (e.g., directional data, graphs, etc.). In such cases, component-wise averaging might produce invalid or meaningless centroids.

Added a note in the BinaryTree docstring (which BallTree inherits) to:
- Clarify this assumption
- Suggest alternatives (brute-force or medoid-based approaches) when the assumption is violated

This helps users understand the limitations when using BallTree with custom metrics."
pull_request,33191,DOC Clarify that Nystroem does not support precomputed kernels,"#### Reference Issues/PRs
Fixes #19178

#### What does this implement/fix?
This PR clarifies in the Nystroem documentation that precomputed kernels are NOT supported, despite 'precomputed' appearing in parameter validation code and `_get_kernel_params` method.

#### Any other comments?
The current implementation is misleading - while ""precomputed"" appears as a valid option in the code, Nystroem cannot actually work with precomputed kernel matrices because it needs to compute the kernel between randomly sampled basis points and input data.

Added a clear note in the docstring to prevent user confusion."
pull_request,33190,DOC Improve var_smoothing parameter documentation in GaussianNB,"This PR addresses #14054 by expanding the documentation for the var_smoothing parameter in GaussianNB.

The enhanced description now explains:
- What the parameter does: prevents zero or very small variances that cause numerical issues
- How it's calculated: `var_smoothing * max(var(X, axis=0))`
- Where the result is stored: `epsilon_` attribute
- The trade-off: larger values provide more smoothing but may reduce accuracy
- That the default value of 1e-9 works well for most cases

This provides users with sufficient detail to understand when and how to adjust this parameter, addressing the ""not documented"" part of the issue."
pull_request,33189,DOC Explain multi-output behavior in GaussianProcessRegressor,"This PR addresses #13989 by adding a Notes section to the GaussianProcessRegressor docstring explaining how multi-output regression is handled.

The documentation now clarifies that:
- Each output dimension is treated independently
- The model does NOT perform co-kriging or multi-task learning
- It does NOT model correlations between different output dimensions
- It maximizes the sum of log-marginal likelihoods across all outputs (equation 5.8 in Rasmussen & Williams 2006)

This answers the original question from @bread9 and provides the technical details identified by @JSestito in the comments."
pull_request,33188,Fix thread safety buffer issue without unnecessary memory allocations,"<!--
üôå Thanks for contributing a pull request!

üëÄ Please ensure you have taken a look at the contribution guidelines:
https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md

‚úÖ In particular following the pull request checklist will increase the likelihood
of having maintainers review your PR:
https://scikit-learn.org/dev/developers/contributing.html#pull-request-checklist

üìã If your PR is likely to affect users, you will need to add a changelog entry
describing your PR changes, see:
https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/README.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
This PR fixes nondeterministic behavior of LocalOutlierFactor with the
Mahalanobis distance when n_jobs > 1, as reported in #32753.

The internal buffer was originally introduced to avoid repeated memory
allocations during distance computations. A naive fix for the reported issue
would be to disable the buffer or reallocate it on each call, which would avoid
the correctness issue but defeat the original performance optimization.

Instead, this PR preserves the buffer-based optimization while making it safe
under parallel execution. This ensures deterministic and correct results without
reintroducing unnecessary memory allocations or performance regressions.

Regression tests are added to verify that:

Parallel execution matches serial execution.

Repeated parallel runs are stable and deterministic.

#### AI usage disclosure
<!--
If AI tools were involved in creating this PR, please check all boxes that apply
below and make sure that you adhere to our Automated Contributions Policy:
https://scikit-learn.org/dev/developers/contributing.html#automated-contributions-policy
-->
I used AI assistance for:
- [x] Code generation (e.g., when writing an implementation or fixing a bug)
- [x] Test/benchmark generation
- [x] Documentation (including examples)
- [x] Research and understanding


#### Any other comments?
No, have a good day.

<!--
Thank you for your patience. Changes to scikit-learn require careful
attention, but with limited maintainer time, not every contribution can be reviewed
quickly.
For more information and tips on improving your pull request, see:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
"
comment,33188,,"Please take the time to fill out the Pull Request template. This should be part of opening a PR, even a draft/early PR. Without the description the maintainers have no context for your work. Without context it is hard to tell spam from ham. This means your PR will be closed without receiving any attention."
comment,33188,,Done. The PR description is fixed. 
pull_request,33187,DOC Add warning about LocalOutlierFactor labelling behavior,"This PR addresses #20989 by adding a Notes section to the LocalOutlierFactor docstring.

The documentation now warns users that binary labeling can produce unexpected results in edge cases (see #20833 for specific example) when many samples have identical LOF scores near the contamination threshold.

The Notes section advises users to:
- Examine raw LOF scores via `negative_outlier_factor_` to verify outlier characteristics
- Adjust the `contamination` parameter if needed
- Apply custom thresholding based on LOF scores for more control

This provides users with actionable guidance when encountering unpredictable labeling outcomes, as requested in the original issue."
issue,33186,scikit-learn 1.8.1 release,"Nothing really major in the [1.8.1 milestone](https://github.com/scikit-learn/scikit-learn/milestone/70) so far. Feel free to add something you are aware of something else to the milestone.

Last monthly meeting (January 26) consensus was to wait 2-3 weeks in case there are side-effects of the pandas 3.0 release (released January 21). I have set the milestone date to beginning of March.

"
pull_request,33185,MahalanobisDistance metric issue resolve,"issue: #32753

What does this implement/fix? Explain your changes.

Previously MahalanobisDistance class used a shared instance variable self.buffer to store intermidiate calculations which resulted in one thread overwriting other threads data so I have replaced the shared instance buffer , now every thread getting its own temporary buffer using malloc so now thread cannot overwrite each other data.

AI usage disclosure

I used AI assistance for:

- [ ] Code generation (e.g., when writing an implementation or fixing a bug)
- [x] Test/benchmark generation
- [ ] Documentation (including examples)
- [x] Research and understanding

Any other comments?
No

@lesteve @adrinjalali  "
comment,33185,,"Please take the time to fill out the Pull Request template. This should be part of opening a PR, even a draft/early PR. Without the description the maintainers have no context for your work. Without context it is hard to tell spam from ham. This means your PR will be closed without receiving any attention."
comment,33185,,@betatim I have edited the PR as per PR template plz check if its alright now.
pull_request,33184,DOC add link to plot_quantile_regression example in QuantileRegressor docstring,"Towards #30621

This PR adds a link to the plot_quantile_regression.py example in the QuantileRegressor class docstring.

The example demonstrates how quantile regression can predict non-trivial conditional quantiles for datasets with heteroscedastic or asymmetric error distributions. This showcases the key use case for QuantileRegressor and helps users understand its advantages over standard regression."
comment,33184,,"Hi @jaideepj2004,

thanks for your work, but #30621 had been closed. We don't take any more PRs on this.
Therefore I will close your PRs without review."
pull_request,33183,DOC add link to plot_theilsen example in TheilSenRegressor docstring,"Towards #30621

This PR adds a link to the plot_theilsen.py example in the TheilSenRegressor class docstring.

The example demonstrates how Theil-Sen regression is robust against outliers by comparing it with OLS and RANSAC on datasets with outliers. This comparison showcases the key advantage of TheilSenRegressor and helps users understand when to use it."
comment,33183,,"Closing, see: https://github.com/scikit-learn/scikit-learn/pull/33184#issuecomment-3835460637."
pull_request,33182,DOC add link to plot_multi_task_lasso_support example in MultiTaskLasso docstring,"Towards #30621

This PR adds a link to the plot_multi_task_lasso_support.py example in the MultiTaskLasso class docstring.

The example demonstrates joint feature selection across multiple tasks, which is the key feature of MultiTaskLasso - enforcing the same features to be selected across all tasks. This link helps users discover this practical example directly from the API documentation."
comment,33182,,"Closing, see: https://github.com/scikit-learn/scikit-learn/pull/33184#issuecomment-3835460637."
pull_request,33181,DOC add link to plot_omp example in OrthogonalMatchingPursuit docstring,"Towards #30621

This PR adds a link to the plot_omp.py example in the OrthogonalMatchingPursuit class docstring.

The example demonstrates using Orthogonal Matching Pursuit for sparse signal recovery from noisy measurements, which is the primary use case for OrthogonalMatchingPursuit. This link helps users discover this practical example directly from the API documentation."
comment,33181,,"Closing, see: https://github.com/scikit-learn/scikit-learn/pull/33184#issuecomment-3835460637."
pull_request,33180,DOC add link to plot_lasso_dense_vs_sparse_data in Lasso docstring,"Towards #30621

This PR adds a link to the `plot_lasso_dense_vs_sparse_data.py` example in the Lasso class docstring, specifically in the `precompute` parameter description.

The example demonstrates performance differences between dense and sparse data formats, which is directly relevant to the `precompute` parameter that behaves differently for sparse data.

This helps users understand when to use sparse vs dense data formats with Lasso."
comment,33180,,"Closing, see: https://github.com/scikit-learn/scikit-learn/pull/33184#issuecomment-3835460637."
pull_request,33179,FIX `pip-tools` error in automatic main lock-file update,"<!--
üôå Thanks for contributing a pull request!

üëÄ Please ensure you have taken a look at the contribution guidelines:
https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md

‚úÖ In particular following the pull request checklist will increase the likelihood
of having maintainers review your PR:
https://scikit-learn.org/dev/developers/contributing.html#pull-request-checklist

üìã If your PR is likely to affect users, you will need to add a changelog entry
describing your PR changes, see:
https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/README.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes #33174

#### What does this implement/fix? Explain your changes.
Adds ""pip=25.3"" in pip-based lock file creation as a workaround for `pip-tools` not being compatible with the latest pip release (26.0) yet (see https://github.com/jazzband/pip-tools/issues/2319).

With this fix, `python build_tools/update_environments_and_lock_files.py` runs without error locally for `debian_32` and `ubuntu_atlas`.
#### AI usage disclosure
<!--
If AI tools were involved in creating this PR, please check all boxes that apply
below and make sure that you adhere to our Automated Contributions Policy:
https://scikit-learn.org/dev/developers/contributing.html#automated-contributions-policy
-->
I used AI assistance for:
- [ ] Code generation (e.g., when writing an implementation or fixing a bug)
- [ ] Test/benchmark generation
- [ ] Documentation (including examples)
- [ ] Research and understanding


#### Any other comments?


<!--
Thank you for your patience. Changes to scikit-learn require careful
attention, but with limited maintainer time, not every contribution can be reviewed
quickly.
For more information and tips on improving your pull request, see:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
"
comment,33179,,"For completeness I double-checked that the following command now works:
```
python build_tools/update_environments_and_lock_files.py --select-build debian_32 -vvv
```

Let's double-check next Monday morning that the automatic lock-file update works!"
pull_request,33178,DOC add link to plot_huber_vs_ridge example in HuberRegressor docstring,"Towards #30621

This PR adds a link to the `plot_huber_vs_ridge.py` example in the HuberRegressor class docstring.

The example demonstrates a comparison between HuberRegressor and Ridge on a dataset with outliers, which is a key use case for HuberRegressor. This link helps users discover this practical example directly from the API documentation.

Similar pattern exists in Ridge class which links to `plot_ridge_coeffs.py` in its docstring."
comment,33178,,"Closing, see: https://github.com/scikit-learn/scikit-learn/pull/33184#issuecomment-3835460637."
pull_request,33176,FIX: StratifiedGroupKFold errors when n_splits > n_groups,"Towards #33085

### What does this PR do?
- Raise a ValueError in StratifiedGroupKFold when `n_splits` is greater than the number of unique groups.
  This aligns StratifiedGroupKFold with GroupKFold and prevents degenerate (empty) train/test splits in
  impossible configurations.

### Why is this needed?
Issue #33085 reports that StratifiedGroupKFold can produce empty / degenerate splits when the number of groups
is too small for the requested number of folds. In such cases, splitting is not feasible, so we fail early with
a clear error message.

### What changed?
- sklearn/model_selection/_split.py:
  - added a check `n_splits > n_groups` and raise ValueError with a descriptive message.
- sklearn/model_selection/tests/test_split.py:
  - test_group_kfold: assert the same failure mode for both GroupKFold and StratifiedGroupKFold.
  - test_2d_y: increase the number of generated groups so the smoke test uses a valid configuration for the default
    StratifiedGroupKFold(n_splits=5).

### Tests
- python -m pytest sklearn/model_selection/tests/test_split.py -vv

### TODO
- [ ] Run full CI (GitHub Actions) and address any failures.
- [ ] Add/confirm a minimal regression example matching #33085.
- [ ] Mark as ""Ready for review"" when complete.
"
pull_request,33175,CI Use a virtual package spec file for `conda-lock` solving,"#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Closes #33106

#### What does this implement/fix? Explain your changes.
This modifies the environment solving/creation of lockfile for our CUDA CI jobs. Newer versions of packages like PyTorch require a virtual conda package called `__cuda` which signals that CUDA is available on the system (same mechanism exists for `__glibc`). This means that solving the environment on a machine that does not have CUDA (our CI job to update the lockfile, a developers local machine, etc) is not possible. I think this is why we were ""stuck"" on a fairly old version of PyTorch (older versions didn't have this dependency). I noticed this problem only when trying to add a new package to the environment (`cuvs`), which is needed for #33096. I think for `cuvs` there are no versions (or only super old ones?) that do not depend on `__cuda`. So solving the environment failed.

#### AI usage disclosure
<!--
If AI tools were involved in creating this PR, please check all boxes that apply
below and make sure that you adhere to our Automated Contributions Policy:
https://scikit-learn.org/dev/developers/contributing.html#automated-contributions-policy
-->
I used AI assistance for:
- [x] Code generation (e.g., when writing an implementation or fixing a bug)
- [ ] Test/benchmark generation
- [ ] Documentation (including examples)
- [x] Research and understanding


#### Any other comments?

What do you think about adding `cuvs` in this PR already? It is needed in cupy for spatial distance functions like `cdist`. Without cuvs installed those functions fail when you call them."
comment,33175,,"The PCA common test failure is weird: it fails for pytorch float32 on CPU, but the same test does not fail for pytorch float32 on cuda device, nor for float32 numpy or array-api-strict on CPU.

Maybe we should use `xp.linalg.pinv` instead of `xp.linalg.inv`? Would be worth investigating interactively locally to understand what's going on."
comment,33175,,"Unfortunately, I cannot reproduce locally with pytorch 2.10.0:

```
sklearn/tests/test_common.py::test_estimators[PCA()-check_array_api_input(array_namespace=torch,dtype_name=float32,device=cpu)] PASSED
```

It's probably architecture dependent."
comment,33175,,"Also the CUDA CI fails see [build log](https://github.com/scikit-learn/scikit-learn/actions/runs/21585960366/job/62194373455?pr=33175)
```
FAILED conda/envs/sklearn/lib/python3.13/site-packages/sklearn/tests/test_common.py::test_estimators[PCA()-check_array_api_input(array_namespace=torch,dtype_name=float32,device=cpu)] - torch._C._LinAlgError: linalg.inv: The diagonal element 10 is zero, the inversion could not be completed because the input matrix is singular.
```"
comment,33175,,"I tried the following locally and our test suite still passes:

```diff
--- a/sklearn/decomposition/_base.py
+++ b/sklearn/decomposition/_base.py
@@ -74,12 +74,12 @@ class _BasePCA(
             return xp.eye(n_features) / self.noise_variance_
 
         if is_array_api_compliant:
-            linalg_inv = xp.linalg.inv
+            linalg_pinv = xp.linalg.pinv
         else:
-            linalg_inv = linalg.inv
+            linalg_pinv = linalg.pinv
 
         if self.noise_variance_ == 0.0:
-            return linalg_inv(self.get_covariance())
+            return linalg_pinv(self.get_covariance())
 
         # Get precision using matrix inversion lemma
         components_ = self.components_
@@ -94,7 +94,7 @@ class _BasePCA(
         )
         precision = components_ @ components_.T / self.noise_variance_
         _add_to_diagonal(precision, 1.0 / exp_var_diff, xp)
-        precision = components_.T @ linalg_inv(precision) @ components_
+        precision = components_.T @ linalg_pinv(precision) @ components_
         precision /= -(self.noise_variance_**2)
         _add_to_diagonal(precision, 1.0 / self.noise_variance_, xp)
         return precision
```

but maybe we should decouple the two problems (dealing with cuda dependencies and fixing `PCA.get_prediction`) independently. We could mark `check_array_api_input` as `XFAIL` for `PCA` (in `PER_ESTIMATOR_XFAIL_CHECKS`) temporarily in this PR and open a tracking issue to fix it separately."
comment,33175,,I relabeled CUDA CI on the last commit to make it clear that there is still a CI failure to handle before considering a merge.
comment,33175,,I've marked it as xfail. Interesting that it fails on the CPU device.
comment,33175,,@lesteve shall we merge?
issue,33174,Automatic main lock-file update fails for debian-32 build,"I can reproduce locally, no clue where this is coming from:
```
python build_tools/update_environments_and_lock_files.py --select-build debian_32 -vvv
```

[build log](https://github.com/scikit-learn/scikit-learn/actions/runs/21578510209/job/62170869669)
```
Traceback (most recent call last):
  File ""/home/runner/work/scikit-learn/scikit-learn/build_tools/update_environments_and_lock_files.py"", line 792, in <module>
    main()
    ~~~~^^
  File ""/usr/share/miniconda/lib/python3.13/site-packages/click/core.py"", line 1442, in __call__
    return self.main(*args, **kwargs)
           ~~~~~~~~~^^^^^^^^^^^^^^^^^
  File ""/usr/share/miniconda/lib/python3.13/site-packages/click/core.py"", line 1363, in main
    rv = self.invoke(ctx)
  File ""/usr/share/miniconda/lib/python3.13/site-packages/click/core.py"", line 1226, in invoke
    return ctx.invoke(self.callback, **ctx.params)
           ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/share/miniconda/lib/python3.13/site-packages/click/core.py"", line 794, in invoke
    return callback(*args, **kwargs)
  File ""/home/runner/work/scikit-learn/scikit-learn/build_tools/update_environments_and_lock_files.py"", line 788, in main
    write_all_pip_lock_files(filtered_pip_build_metadata_list)
    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/scikit-learn/scikit-learn/build_tools/update_environments_and_lock_files.py"", line 672, in write_all_pip_lock_files
    write_pip_lock_file(build_metadata)
    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File ""/home/runner/work/scikit-learn/scikit-learn/build_tools/update_environments_and_lock_files.py"", line 666, in write_pip_lock_file
    pip_compile(pip_compile_path, requirement_path, lock_file_path)
    ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/scikit-learn/scikit-learn/build_tools/update_environments_and_lock_files.py"", line 621, in pip_compile
    execute_command(
    ~~~~~~~~~~~~~~~^
        [
        ^
    ...<5 lines>...
        ]
        ^
    )
    ^
  File ""/home/runner/work/scikit-learn/scikit-learn/build_tools/update_environments_and_lock_files.py"", line 476, in execute_command
    raise RuntimeError(
    ...<5 lines>...
    )
RuntimeError: Command exited with non-zero exit code.
Exit code: 1
Command:
/usr/share/miniconda/envs/pip-tools-python3.12.5/bin/pip-compile --upgrade build_tools/azure/debian_32bit_requirements.txt -o build_tools/azure/debian_32bit_lock.txt
stdout:

stderr:
Traceback (most recent call last):
  File ""/usr/share/miniconda/envs/pip-tools-python3.12.5/bin/pip-compile"", line 10, in <module>
    sys.exit(cli())
             ^^^^^
  File ""/usr/share/miniconda/envs/pip-tools-python3.12.5/lib/python3.12/site-packages/click/core.py"", line 1485, in __call__
    return self.main(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/share/miniconda/envs/pip-tools-python3.12.5/lib/python3.12/site-packages/click/core.py"", line 1406, in main
    rv = self.invoke(ctx)
         ^^^^^^^^^^^^^^^^
  File ""/usr/share/miniconda/envs/pip-tools-python3.12.5/lib/python3.12/site-packages/click/core.py"", line 1269, in invoke
    return ctx.invoke(self.callback, **ctx.params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/share/miniconda/envs/pip-tools-python3.12.5/lib/python3.12/site-packages/click/core.py"", line 824, in invoke
    return callback(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/share/miniconda/envs/pip-tools-python3.12.5/lib/python3.12/site-packages/click/decorators.py"", line 34, in new_func
    return f(get_current_context(), *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/share/miniconda/envs/pip-tools-python3.12.5/lib/python3.12/site-packages/piptools/scripts/compile.py"", line 475, in cli
    prereleases=repository.finder.allow_all_prereleases or pre,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'PackageFinder' object has no attribute 'allow_all_prereleases'
```

cc @AnneBeyer if you want to have a closer look üôè."
comment,33174,,"I notice a similar error after upgrading `pip` to 26.  I suspect pip-tools*** that provides the `pip_compile` command is not yet compatible with this new release.  
 
*** I am using pip-tools version 7.5.2.  I plan to roll back to pip 25.3 for now."
comment,33174,,Just checked Issues for pip-tools and see: https://github.com/jazzband/pip-tools/issues/2319
comment,33174,,"I can confirm what @asset-web found, the release of pip 26.0 on January 30 is causing this issue in pip-tools."
comment,33174,,"All right, thanks! A PR would be very welcome to temporarily pin the pip version `pip<26` in https://github.com/scikit-learn/scikit-learn/blob/cedf994f1e0c113bb5f905fb39cb56df9d86f3b8/build_tools/update_environments_and_lock_files.py#L641-L651.

Also adding a TODO comment to remove this temporarily pin until the pip-tools issue is fixed with a link to the pip-tools issue.

"
comment,33174,,Thank you for the pointer! I was just trying to figure out where this has to be added.
comment,33174,,Thanks @asset-web for finding the relevant issue and @AnneBeyer for the PR!
comment,33174,,"> I notice a similar error after upgrading `pip` to 26. I suspect pip-tools*** that provides the `pip_compile` command is not yet compatible with this new release.
> 
> *** I am using pip-tools version 7.5.2. I plan to roll back to pip 25.3 for now.

TY, this solved an issue I was having in my own development!"
pull_request,33173,:lock: :robot: CI Update lock files for array-api CI build(s) :lock: :robot:,"Update lock files.

### Note
If the CI tasks fail, create a new branch based on this PR and add the required fixes to that branch."
comment,33173,,Closing in favor of #33175 which seems to be the right way to deal with cuda dependencies.
pull_request,33172,:lock: :robot: CI Update lock files for free-threaded CI build(s) :lock: :robot:,"Update lock files.

### Note
If the CI tasks fail, create a new branch based on this PR and add the required fixes to that branch."
pull_request,33171,:lock: :robot: CI Update lock files for scipy-dev CI build(s) :lock: :robot:,"Update lock files.

### Note
If the CI tasks fail, create a new branch based on this PR and add the required fixes to that branch."
pull_request,33170,FIX: Correct response method handling for regressors in _response.py,"<!--
üôå Thanks for contributing a pull request!

üëÄ Please ensure you have taken a look at the contribution guidelines:
https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md

‚úÖ In particular following the pull request checklist will increase the likelihood
of having maintainers review your PR:
https://scikit-learn.org/dev/developers/contributing.html#pull-request-checklist

üìã If your PR is likely to affect users, you will need to add a changelog entry
describing your PR changes, see:
https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/README.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.


#### AI usage disclosure
<!--
If AI tools were involved in creating this PR, please check all boxes that apply
below and make sure that you adhere to our Automated Contributions Policy:
https://scikit-learn.org/dev/developers/contributing.html#automated-contributions-policy
-->
I used AI assistance for:
- [ ] Code generation (e.g., when writing an implementation or fixing a bug)
- [ ] Test/benchmark generation
- [ ] Documentation (including examples)
- [ ] Research and understanding


#### Any other comments?


<!--
Thank you for your patience. Changes to scikit-learn require careful
attention, but with limited maintainer time, not every contribution can be reviewed
quickly.
For more information and tips on improving your pull request, see:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
"
comment,33170,,"## ‚ùå Linting issues

This PR is introducing linting issues. Here's a summary of the issues. Note that you can avoid having linting issues by enabling `pre-commit` hooks. Instructions to enable them can be found [here](https://scikit-learn.org/dev/developers/development_setup.html#set-up-pre-commit).

You can see the details of the linting issues under the `lint` job [here](https://github.com/scikit-learn/scikit-learn/actions/runs/21593829873)

-----------------------------------------------
### `ruff check`

`ruff` detected issues. Please run `ruff check --fix --output-format=full` locally, fix the remaining issues, and push the changes. Here you can see the detected issues. Note that the installed `ruff` version is `ruff=0.12.2`.

<details>

```

sklearn/utils/_response.py:89:89: E501 Line too long (91 > 88)
   |
87 |     return_response_method_used=False,
88 | ):
89 |     """"""Compute the response values of a classifier, an outlier detector, or a regressor.""""""
   |                                                                                         ^^^ E501
90 |
91 |     if is_classifier(estimator):
   |

Found 1 error.
```

</details>



<sub> _Generated for commit: [9d2a572](https://github.com/scikit-learn/scikit-learn/pull/33170/commits/9d2a5721402f719201dd7a0c02741c3f130363b6). Link to the linter CI: [here](https://github.com/scikit-learn/scikit-learn/actions/runs/21593829873)_ </sub>"
comment,33170,,"This PR does not meet *any* of our requirements, and this problem is already being addressed here https://github.com/scikit-learn/scikit-learn/pull/33126."
comment,33170,,"Hello maintainers, 

I noticed this documentation section could be improved/clarified. 
I‚Äôm a student contributor and would like to submit a PR for this. 
Any suggestions or preferences are welcome! 

Thanks in advance.
"
comment,33170,,"Closing this, see https://github.com/scikit-learn/scikit-learn/pull/33170#issuecomment-3833932430

@AdoDeBrilliance thanks for being interested in contributing to an open-source project! From your profile it looks like you are new to GitHub and contributing to open-source. scikit-learn is not a good match for contributors without significant experience contributing to large open-source projects. I recommend finding another project that you actively use that is smaller and contributing there. Once you have gained experience contributing to open-source projects you are welcome to help with scikit-learn. We have a guide for contributors that makes recommendations on where to start https://scikit-learn.org/stable/developers/contributing.html#ways-to-contribute"
pull_request,33169,DOC: Add reference links to hinge loss API documentation,"<!--
üôå Thanks for contributing a pull request!

üëÄ Please ensure you have taken a look at the contribution guidelines:
https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md

‚úÖ In particular following the pull request checklist will increase the likelihood
of having maintainers review your PR:
https://scikit-learn.org/dev/developers/contributing.html#pull-request-checklist

üìã If your PR is likely to affect users, you will need to add a changelog entry
describing your PR changes, see:
https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/README.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.

Add reference links to hinge loss API documentation

#### AI usage disclosure
<!--
If AI tools were involved in creating this PR, please check all boxes that apply
below and make sure that you adhere to our Automated Contributions Policy:
https://scikit-learn.org/dev/developers/contributing.html#automated-contributions-policy
-->
I used AI assistance for:
- [ ] Code generation (e.g., when writing an implementation or fixing a bug)
- [ ] Test/benchmark generation
- [ ] Documentation (including examples)
- [ ] Research and understanding


#### Any other comments?


<!--
Thank you for your patience. Changes to scikit-learn require careful
attention, but with limited maintainer time, not every contribution can be reviewed
quickly.
For more information and tips on improving your pull request, see:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
"
pull_request,33168,FIX _predict_proba_lr in LinearClassifierMixin,"#### Reference Issues/PRs
Fixes #17978.

#### What does this implement/fix? Explain your changes.
`LinearClassifierMixin._predict_proba_lr` might return nan or inf if `decision_function` returns large negative values, because all probabilities are zero in floating point arithmetic (e.g. exp(-1000) = 0).

#### AI usage disclosure
None

#### Any other comments?
I guess the only estimator is `SGDClassifier`, not 100% sure."
comment,33168,,"> I guess the only estimator is SGDClassifier, not 100% sure.

`_predict_proba_lr` is also called by `LogisticRegression.predict_proba` in the binary classification case. I think, the changelog should be updated accordingly.

EDIT: actually, the change in this PR only impacts the multiclass branch of the `_predict_proba_lr` method, hence this cannot impact `LogisticRegression`."
comment,33168,,"> EDIT: actually, the change in this PR only impacts the multiclass branch of the _predict_proba_lr method, hence this cannot impact LogisticRegression.

That's the benefit of having gotten rid of one-vs-rest multiclass in LogisticRegression üéâ 

Thanks for the fast reviews!"
